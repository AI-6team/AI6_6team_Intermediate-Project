{
  "meta": {
    "experiment": "Exp-03 Prompt Engineering",
    "version": "v1",
    "date": "2026-02-09T15:34:25.149263",
    "sample_file": "../data/raw/files\\고려대학교_차세대 포털·학사 정보시스템 구축사업.pdf",
    "num_test_cases": 30,
    "prerequisite": {
      "chunk_size": 500,
      "table_strategy": "layout",
      "alpha": 0.3,
      "top_k": 15,
      "exp02_context_recall": 0.7666666666666667
    }
  },
  "best_config": {
    "strategy": "zero_shot_ko",
    "description": "한국어 Zero-shot",
    "faithfulness": 0.9632369720605015,
    "answer_relevancy": 0.4170788409368907,
    "context_recall": 0.7333333333333333,
    "keyword_accuracy": 0.4916201302424212
  },
  "strategies": [
    "zero_shot_en",
    "zero_shot_ko",
    "few_shot_ko",
    "cot_ko"
  ],
  "results": [
    {
      "strategy": "zero_shot_en",
      "description": "Baseline (영문 Zero-shot)",
      "faithfulness": 0.8647140768588136,
      "answer_relevancy": 0.5896066066569967,
      "context_recall": 0.7,
      "keyword_accuracy": 0.44034174488508854,
      "gen_time": 253.32040740002412,
      "ragas_time": 582.7194185000262,
      "latency_total": 836.0404804000282
    },
    {
      "strategy": "zero_shot_ko",
      "description": "한국어 Zero-shot",
      "faithfulness": 0.9632369720605015,
      "answer_relevancy": 0.4170788409368907,
      "context_recall": 0.7333333333333333,
      "keyword_accuracy": 0.4916201302424212,
      "gen_time": 286.80080469994573,
      "ragas_time": 573.6954601999605,
      "latency_total": 860.4965778999613
    },
    {
      "strategy": "few_shot_ko",
      "description": "한국어 Few-shot (2 examples)",
      "faithfulness": 0.9110611185201348,
      "answer_relevancy": 0.3940303734783542,
      "context_recall": 0.7666666666666667,
      "keyword_accuracy": 0.41102609464838563,
      "gen_time": 266.938570600003,
      "ragas_time": 542.0967310999986,
      "latency_total": 809.0356293999939
    },
    {
      "strategy": "cot_ko",
      "description": "한국어 Chain-of-Thought",
      "faithfulness": NaN,
      "answer_relevancy": 0.5310098887468963,
      "context_recall": 0.7666666666666667,
      "keyword_accuracy": 0.6065122457808216,
      "gen_time": 424.3870238000527,
      "ragas_time": 719.8813513999921,
      "latency_total": 1144.2689540000283
    }
  ]
}