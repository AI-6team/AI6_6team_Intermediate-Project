# BidFlow 중간 보고서 (발표/토의용)

**프로젝트명**: BidFlow - 보안 강화형 지능형 입찰 분석 시스템
**슬로건**: *Don't just Write, Find & Verify.*
**보고일**: 2026-02-11
**작성팀**: AI6기 6팀

---

> 상세 기획은 `AI6기_6팀_중급프로젝트_기획서.md`, 시각화는 `notebooks/intermediate_report.ipynb`를 참조하세요.

---

## 용어 해설 (Glossary)

본 보고서에서 사용하는 주요 용어를 먼저 정리합니다.

### 일반 용어

| 용어 | 영문 | 설명 |
|:---|:---|:---|
| **RFP** | Request for Proposal | 제안요청서. 발주처가 입찰 참여자에게 제시하는 요구사항 문서 |
| **RAG** | Retrieval-Augmented Generation | 검색 증강 생성. 외부 문서에서 관련 정보를 검색한 뒤, 그 정보를 바탕으로 LLM이 답변을 생성하는 방식 |
| **LLM** | Large Language Model | 대규모 언어 모델. GPT 시리즈 등 자연어를 이해하고 생성하는 AI 모델 |
| **Compliance Matrix** | - | 적합성 매트릭스. RFP에서 추출해야 할 30개 필수 항목(슬롯)의 구조화된 체크리스트 |
| **슬롯 (Slot)** | - | Compliance Matrix의 개별 항목. 예: "사업예산", "입찰자격", "배점 비율" 등 |
| **GRAY (판단 유보)** | - | 근거가 부족하거나 파싱 신뢰도가 낮을 때, GREEN/RED 대신 "판단 유보"로 안전하게 처리하는 상태 |
| **Golden Testset** | - | 사람이 직접 정답을 라벨링한 평가용 데이터셋. 현재 30문항으로 구성 |

### 기획/요구사항 용어

| 용어 | 영문 | 설명 |
|:---|:---|:---|
| **FR** | Functional Requirement | 기능 요구사항. 시스템이 "무엇을 해야 하는지" 정의 (예: FR-01 = RFP 파일 업로드) |
| **NFR** | Non-Functional Requirement | 비기능 요구사항. 시스템의 "품질/성능/보안 기준" 정의 (예: NFR-01 = 응답시간 30초 이내) |
| **MVP** | Minimum Viable Product | 최소 기능 제품. 핵심 기능만 갖춘 첫 번째 동작 가능 버전 |
| **PDCA** | Plan-Do-Check-Act | 계획-실행-점검-개선 사이클. 반복적 품질 개선 방법론 |
| **ADR** | Architecture Decision Record | 아키텍처 의사결정 기록. 기술 선택의 이유를 문서화한 것 |

### 기술 용어

| 용어 | 영문 | 설명 |
|:---|:---|:---|
| **Chunking** | - | 긴 문서를 LLM이 처리할 수 있는 작은 단위(chunk)로 나누는 작업 |
| **Embedding** | - | 텍스트를 수치 벡터로 변환하는 과정. 의미가 비슷한 텍스트는 비슷한 벡터값을 가짐 |
| **BM25** | Best Match 25 | 키워드 기반 검색 알고리즘. 특정 단어가 문서에 얼마나 자주/중요하게 등장하는지 점수를 매김 |
| **Hybrid Search** | - | BM25(키워드)와 Vector(의미) 검색을 결합한 방식. Alpha 값으로 비중을 조절 |
| **Alpha (α)** | - | Hybrid Search에서 BM25와 Vector 검색의 혼합 비율. α=0은 BM25만, α=1은 Vector만 사용 |
| **Reranker** | - | 1차 검색 결과를 더 정교한 모델로 다시 순위를 매기는 장치. 후보군에서 진짜 관련 있는 문서를 상위로 올림 |
| **Cross-encoder** | - | Reranker의 한 방식. 질문과 문서를 함께 넣어 관련성을 직접 판단하므로 정확도가 높음 (대신 느림) |
| **Top-K** | - | 검색 결과에서 상위 K개만 사용하는 것. 예: top-15 = 상위 15개 문서만 LLM에 전달 |
| **RRF** | Reciprocal Rank Fusion | 여러 검색 결과의 순위를 결합하는 방법. 각 결과의 역순위를 합산하여 최종 순위를 정함 |
| **Ablation Study** | - | 시스템의 구성 요소를 하나씩 제거/추가하며 각각의 기여도를 측정하는 실험 방법 |
| **3-Rail** | - | 보안 아키텍처. Input(입력 차단) + Process(처리 중 보호) + Output(출력 검증) 3단계 방어 |

### 평가 지표 용어

| 지표 | 영문 | 의미 | 쉬운 설명 |
|:---|:---|:---|:---|
| **Context Recall (CR)** | - | 정답을 찾기 위해 필요한 문서 조각들 중, 검색이 실제로 찾아온 비율 | "필요한 정보를 얼마나 빠짐없이 검색했는가?" (높을수록 좋음) |
| **Context Precision** | - | 검색된 문서 조각들 중, 실제로 정답에 도움이 되는 비율 | "검색 결과에 쓸모없는 것이 얼마나 섞여 있는가?" (높을수록 좋음) |
| **Keyword Accuracy (KW Acc)** | - | 생성된 답변의 핵심 키워드가 정답과 얼마나 일치하는지 | "추출한 답의 핵심 단어가 정답과 맞는가?" (높을수록 좋음) |
| **Faithfulness** | - | 생성된 답변이 검색된 문서의 내용에 근거하고 있는 비율 | "LLM이 지어낸 게 아니라 실제 문서에 있는 내용인가?" (높을수록 좋음, 환각 방지 지표) |
| **MRR** | Mean Reciprocal Rank | 정답 문서가 검색 결과 중 몇 번째에 나타나는지의 역수 평균 | "정답이 검색 결과 상위에 있는가?" (높을수록 좋음) |
| **Slot Omission Rate** | - | 30개 필수 슬롯 중 정답 대비 누락된 비율 | "필수 항목을 얼마나 빠뜨렸는가?" (낮을수록 좋음, <10% 목표) |
| **Oracle Recall** | - | 인덱스(검색 가능한 전체 저장소)에 정답 정보가 존재하는 비율 | "애초에 검색 대상에 정답이 있기는 한가?" (검색 성능의 이론적 상한) |
| **Latency** | - | 요청부터 응답까지 걸리는 시간 | "얼마나 빨리 결과가 나오는가?" (낮을수록 좋음) |

---

## 1. 기획 개요

### 어떤 문제를 풀려고 하는가?

입찰 컨설턴트는 매일 수십 페이지짜리 RFP(제안요청서)를 읽고, 우리 회사가 이 입찰에 참여할 수 있는지 판단해야 합니다. 이 과정에서 가장 위험한 실수는 **필수 자격 요건을 놓치는 것**입니다. "보안 인증이 필요했는데 못 봤다", "실적 기준이 5건이었는데 3건으로 잘못 읽었다" 같은 실수가 실제로 입찰 부적격으로 이어집니다.

BidFlow는 이 문제를 **RAG(검색 증강 생성) 기술**로 해결합니다. 단순히 "요약해 줘"가 아니라, RFP에서 **30개 필수 항목을 구조적으로 추출**하고, **회사 프로필과 자동 비교**하여 Go/No-Go 판정을 내립니다.

### 핵심 설계 원칙

우리 시스템의 가장 중요한 철학은 **"모르면 모른다고 말하기"** 입니다.

1. **근거 강제**: 모든 답변에 반드시 출처(몇 페이지, 어떤 표, 어떤 셀)를 붙입니다
2. **단정 금지**: 근거가 불충분하면 "적격(GREEN)" 또는 "부적격(RED)"이라 단정 짓지 않고 **"판단 유보(GRAY)"** 로 처리합니다
3. **판정 분리**: LLM은 정보 추출만 하고, 최종 판정은 규칙 기반 Validator가 합니다. LLM의 "생각"에 판정을 맡기지 않습니다
4. **보안 우선**: 프롬프트 인젝션 공격 방어, 민감정보 마스킹 등 OWASP 보안 기준 준수

### 요구사항 정리

**기능 요구사항 (FR: Functional Requirements)** - 시스템이 해야 할 일:

| ID | 기능 | 상태 |
|:---:|:---|:---:|
| FR-01 | PDF/HWP 파일 업로드 및 문서 관리 | 구현 완료 |
| FR-02 | 텍스트/표/메타데이터 파싱 (레이아웃 보존) | 구현 완료 |
| FR-03 | 30개 필수 항목(Compliance Matrix) 추출 | 구현 완료 |
| FR-04 | 회사 프로필과 비교하여 Go/No-Go 판정 | 구현 완료 |
| FR-05 | Streamlit 기반 결과 조회 및 근거 하이라이트 | 구현 완료 |
| FR-06 | 프롬프트 인젝션 방어 및 민감정보 마스킹 | 구현 중 (80%) |

**비기능 요구사항 (NFR: Non-Functional Requirements)** - 시스템의 품질 기준:

| ID | 기준 | 목표 | 현재 |
|:---:|:---|:---|:---|
| NFR-01 | 문서 파싱~추출 완료 시간 | 30초 이내 | 측정 중 |
| NFR-02 | 필수 슬롯 누락률 | 10% 미만 | 실험으로 개선 중 |
| NFR-03 | OWASP Top 10 보안 테스트 | Critical 0건 | 테스트 예정 |
| NFR-04 | 근거 없는 단정(환각) 발생 시 | GRAY 처리 | 구현 완료 |

> 기획서 전문: `AI6기_6팀_중급프로젝트_기획서.md`

---

## 2. 시스템 아키텍처

### 전체 흐름 (6단계 파이프라인)

RFP 문서가 들어오면 다음 순서로 처리됩니다:

```
[1] Ingest     →  [2] Parse      →  [3] Index       →  [4] Extract     →  [5] Validate    →  [6] Serve
(업로드/버전화)   (PDF/HWP 파싱)   (검색 인덱스 생성)   (30개 항목 추출)   (룰 기반 판정)   (UI로 결과 표시)
```

각 단계를 조금 더 풀어 설명하면:

**[1] Ingest**: 사용자가 RFP 파일을 업로드하면, 문서에 고유 ID와 해시값을 부여하여 버전 관리합니다.

**[2] Parse**: PDF 문서에서 텍스트와 표를 추출합니다. 특히 **표(Table)가 깨지지 않도록 레이아웃을 보존**하는 것이 핵심입니다. 입찰 RFP에서 배점표, 일정표 등 표 안에 있는 정보가 매우 중요하기 때문입니다.

**[3] Index**: 추출된 텍스트를 검색할 수 있도록 **Dual-Index**(이중 인덱스)를 생성합니다.
- **Text Index**: 일반 본문 검색용
- **Table Index**: 표 데이터 검색용
- 각 인덱스는 BM25(키워드 검색)와 Vector(의미 검색) 두 가지 방식을 모두 지원합니다.

**[4] Extract**: 질문을 30개 슬롯 그룹별로 순차 실행합니다.
- G1 (기본정보) → G2 (일정/제출) → G3 (자격/결격) → G4 (평가/배점)
- 앞 그룹의 결과를 뒷 그룹의 컨텍스트에 주입합니다 (의존성 연결)
- 예: G1에서 추출한 "사업유형"을 G3의 자격 요건 추출 시 참고

**[5] Validate**: 추출된 결과와 회사 프로필을 **규칙 기반으로 비교**합니다.
- GREEN: 모든 필수 조건 충족 (참여 가능)
- RED: 명확한 부적격 (참여 불가)
- GRAY: 판단 유보 (추가 확인 필요)
- 이 단계는 LLM 없이 순수 룰 비교이므로, **프로필만 수정하면 재추출 없이 즉시 재판정**이 가능합니다.

**[6] Serve**: Streamlit UI로 결과를 시각화합니다. 5개 페이지로 구성:
- 업로드 → Compliance Matrix 조회 → 회사 프로필 편집 → Go/No-Go 판정 → 평가 대시보드

### 보안 레이어 (3-Rail Architecture)

전체 파이프라인을 관통하는 보안 레이어가 있습니다:

- **Input Rail**: 사용자 입력에서 프롬프트 인젝션(악의적 명령어 삽입) 패턴을 탐지하여 차단
- **Process Rail**: LLM API 호출 전 민감 정보(주민번호, 전화번호 등)를 마스킹
- **Output Rail**: LLM 응답에서 근거 없는 단정을 감지하면 GRAY로 다운그레이드

### 기술 스택

| 영역 | 기술 | 선택 이유 |
|:---|:---|:---|
| Backend | FastAPI | 비동기 성능, Pydantic 자동 검증, API 문서 자동 생성 |
| Frontend | Streamlit | Python 기반 빠른 프로토타이핑, 데이터 시각화 용이 |
| LLM 연동 | LangChain | 다양한 LLM/도구 체인 연동 유연성 |
| Vector DB | ChromaDB | 경량 벡터 저장소, 로컬 실행 용이 |
| 평가 | RAGAS | RAG 전용 평가 프레임워크, 표준 지표 제공 |
| 관측성 | Langfuse (예정) | 오픈소스, Step별 트레이스 추적 가능 |

---

## 3. 구현 완료 기능

### 구현 현황 요약

현재 **21개 주요 기능 중 20개 완료**, 1개 진행 중이며 전체 평균 완성도는 **약 95%** 입니다.

#### Domain (도메인 모델)
- [x] **30-슬롯 Compliance Matrix 정의** (G1 기본정보 4개 + G2 일정/제출 6개 + G3 자격/결격 8개 + G4 평가/배점 6개 + 기타 6개)
- [x] **Pydantic 데이터 모델** - Evidence(근거), ExtractionSlot(추출 결과), ComplianceMatrix(전체 매트릭스) 등 타입 안전한 데이터 구조

#### Parsing (문서 파싱)
- [x] **PDF 파싱** - PyMuPDF(텍스트 추출) + pdfplumber(표 추출) 조합
- [x] **HWP 파싱** - 기본 구조 구현 (80%, 검증 예정)
- [x] **보수적 전처리** - 의미 없어 보이는 문장에도 예외 조항이 숨어있을 수 있으므로, 삭제보다 정규화 중심

#### Indexing (인덱싱)
- [x] **Dual-Index** - 텍스트 인덱스와 표 인덱스를 별도 관리
- [x] **BM25 + Vector** - 키워드 검색과 의미 검색 모두 지원

#### Retrieval (검색)
- [x] **Hybrid Search** - BM25와 Vector 검색을 Alpha 값으로 혼합 (Alpha=0.7이 최적)
- [x] **Cross-encoder Reranker** - bge-reranker-v2-m3 모델로 검색 결과 재순위 (90%, 프로덕션 통합 예정)

#### Extraction (추출)
- [x] **Multi-step 파이프라인** - G1→G2→G3→G4 순차 추출, 의존성 주입
- [x] **프롬프트 템플릿** - 각 그룹별 한국어 프롬프트 + Verbatim(원문 그대로) 추출 지시

#### Validation (검증)
- [x] **Rule-based Validator** - 수치/날짜/자격/인력 등 도메인별 비교 규칙
- [x] **GREEN/RED/GRAY 결정 트리** - 상태 전이 로직

#### Security (보안)
- [x] **3-Rail 구조** (80%) - Input/Process/Output Rail 기본 구현
- [x] **PII 마스킹** (70%) - 개인정보 자동 감지 및 마스킹

#### UI (사용자 인터페이스)
- [x] **Streamlit 5페이지 앱** - Upload, Matrix, Profile, Decision, Eval
- [x] **컴포넌트** - 근거 뷰어, 프로필 에디터, 판정 뱃지

#### API (백엔드)
- [x] **FastAPI 라우터** - upload, analyze, validate, query, eval (90%)

#### Evaluation (평가)
- [x] **RAGAS 파이프라인** - Faithfulness, Context Recall, Answer Relevancy 자동 측정
- [x] **Golden Testset** - 30문항 (12개 카테고리), 사람이 직접 정답 라벨링
- [ ] **RAG 성능 최적화 실험** - EXP01~04v3 완료, EXP05~06 예정 (75%)

---

## 4. RAG 성능 최적화 (현재 진행 중)

### 왜 RAG 최적화를 하는가?

MVP를 만들고 보니, **"검색은 되는데 추출 품질이 안 나오는"** 문제가 발생했습니다. 구체적으로:

- 문서에서 정보를 찾아오는 것(검색)은 되지만, 찾아온 정보에서 정확한 값을 뽑아내는 것(추출)이 부정확
- 특히 **표(Table) 안의 숫자/배점 정보**를 잘 못 추출함
- "사업예산이 112.7억원인데 113억으로 답한다" 같은 미세한 오류가 입찰에서는 치명적

이를 해결하기 위해 **실험 계획서(`experiment_plan.md`)에 따라 체계적인 실험**을 진행하고 있습니다.

### 실험 프레임워크

한 번에 여러 변수를 바꾸면 뭐 때문에 좋아졌는지 모르므로, **한 번에 하나의 변수만 변경(One-lever Change)**하는 원칙을 따릅니다.

```
Phase A: Chunking 최적화     →  "문서를 어떻게 자르면 정보 손실이 적을까?"
Phase B: Retrieval 최적화    →  "검색 방식을 어떻게 조합하면 정답을 잘 찾을까?"
Phase C: Prompt 최적화       →  "LLM에게 어떻게 지시하면 정확하게 추출할까?"
Phase D: 통합 파이프라인      →  "위 결과를 종합하여 최적 조합을 찾자"
```

### 실험 경과 (Exp-01 ~ Exp-04v3)

#### Exp-01: Chunking 최적화 (02/06)

**질문**: "문서를 몇 글자 단위로 잘라야 정보 손실이 적은가?"

| 설정 | Context Recall | 비고 |
|:---|:---:|:---|
| chunk=500, table=text | 0.683 | 표를 텍스트로 풀어버림 |
| chunk=1000, table=text | 0.600 | 덩어리가 너무 크면 오히려 하락 |
| **chunk=500, table=layout** | **0.733** | **표 레이아웃 보존이 핵심** |
| chunk=2000, table=layout | 0.667 | 과도한 chunk 크기는 불리 |

**결론**: chunk_size=500 + 표 레이아웃 보존(layout) 방식이 가장 우수. **표가 깨지면 성능이 급락**한다는 것을 실험으로 확인.

---

#### Exp-02: Retrieval 전략 최적화 (02/09)

**질문**: "키워드 검색(BM25)과 의미 검색(Vector)을 어떤 비율로 섞어야 좋은가?"

Alpha 값 0.0(BM25만) ~ 1.0(Vector만)과 Top-K(상위 몇 개를 쓸지)를 조합하여 실험.

| 설정 | Context Recall | MRR | 비고 |
|:---|:---:|:---:|:---|
| BM25만 (α=0), K=15 | 0.733 | 0.377 | 키워드만으로도 꽤 잘 됨 |
| **Hybrid (α=0.5), K=15** | **0.767** | **0.419** | **최적 조합** |
| Vector만 (α=1), K=15 | 0.733 | 0.349 | 의미 검색만으로는 부족 |
| Hybrid (α=0.7), K=10 | 0.736 | 0.392 | K를 줄이면 다소 하락 |

**결론**: Hybrid 방식이 단독 검색보다 우수. α=0.5, K=15가 최적. 입찰 문서는 특수 용어가 많아 키워드 검색(BM25)의 역할이 중요.

---

#### Exp-03: Prompt Engineering (02/09)

**질문**: "LLM에게 한국어/영어 중 어떤 언어로, 어떤 방식으로 지시하면 정확하게 추출하는가?"

| 전략 | Faithfulness | KW Accuracy | 비고 |
|:---|:---:|:---:|:---|
| 영문 Zero-shot | 0.865 | 0.440 | 기본 영문 프롬프트 |
| **한국어 Zero-shot** | **0.963** | **0.492** | **Faithfulness 최고** |
| 한국어 Few-shot (2예시) | 0.911 | 0.411 | 예시를 줘도 KW_Acc 하락 |
| 한국어 CoT | NaN | 0.607 | KW_Acc는 높지만 환각 발생 |

> **Zero-shot**: 예시 없이 지시만 하는 방식
> **Few-shot**: 1~3개 예시를 함께 제공하는 방식
> **CoT (Chain-of-Thought)**: "단계별로 생각해봐"라고 시키는 방식

**결론**: 한국어 Zero-shot이 Faithfulness(환각 방지) 최고. CoT는 KW_Acc가 높지만 Faithfulness 측정 불가(환각 발생)로 위험. **"정확한 값 추출"에는 Zero-shot이 안전**.

---

#### Exp-04: 통합 파이프라인 Enhancement (02/10)

**질문**: "Exp-01~03 결과를 종합하여, 추가 기법들을 하나씩 쌓으면 어떻게 되는가?"

Ablation Study(누적 추가 실험)로 각 기법의 기여도를 측정:

| 설정 | KW Accuracy | Faithfulness | 변화 |
|:---|:---:|:---:|:---|
| A. Baseline (Exp-03 best) | 0.443 | 0.988 | 기준점 |
| B. + Document Metadata | 0.585 | 0.810 | KW +0.142, Faith -0.178 |
| **C. + Verbatim Prompt** | **0.664** | **0.829** | **KW +0.079, Faith 회복** |
| D. + LLM Reranker | 0.652 | 0.852 | 미미한 변화 |
| E. + Query Decomposition | 0.586 | 0.872 | KW 오히려 하락 |
| F. + Full Pipeline | 0.616 | 0.844 | 복잡화 효과 미미 |

> **Verbatim Prompt**: "원문에 나온 그대로 답하라"는 지시. 숫자나 고유명사의 변형을 방지
> **Document Metadata**: 사업명, 발주처 등 문서 정보를 프롬프트에 사전 주입
> **Query Decomposition**: 복합 질문을 단순 질문들로 분해

**결론**: Verbatim Prompt가 가장 효과적 (+0.221). 반면 아키텍처 복잡화(D~F)는 **오히려 Top-K 검색 결과를 오염시켜 역효과**. "복잡하면 좋은 게 아니다"를 학습.

---

#### Exp-04v2: Retrieval 아키텍처 고도화 (02/11 오전)

Exp-04의 교훈(복잡화 역효과)에도 불구하고, Section-aware chunking, Table Dual-Index 등을 시도.

**결과**: 모든 변형이 Baseline보다 **오히려 성능이 떨어짐**. 아키텍처 레벨의 변경보다 **개별 컴포넌트 품질 향상**이 중요하다는 교훈 재확인.

---

#### Exp-04v3: 매칭 품질 직접 개선 (02/11, 현재 최신)

**철학 전환**: "후보를 다양하게 만들자" → **"후보 품질을 직접 올리자"**

이전 실험들의 실패에서 배운 핵심: 복잡한 구조를 더하는 것보다, **검색 결과의 품질 자체를 높이는 것**이 정답.

##### Phase 0: 시스템 진단

먼저 "어디가 문제인지"를 정확히 진단했습니다:

| 진단 항목 | 결과 | 의미 |
|:---|:---:|:---|
| Oracle Recall | 0.917 | 인덱스에 정답이 91.7% 존재 → 인덱싱은 문제 없음 |
| Recall@15 | 0.703 | 상위 15개만 쓰면 70.3%만 찾음 |
| Recall@50 | 0.818 | 상위 50개로 넓히면 81.8% → **순위를 올리면 개선 가능** |
| Gap(15→50) | 0.115 | > 0.1이므로 **Reranker가 효과적**이라 판단 |
| Table Recall@15 | 0.524 | 표 관련 질문은 절반밖에 못 찾음 |
| Text Recall@15 | 0.792 | 텍스트 질문은 양호 |

**진단 결론**: "정보는 인덱스에 있지만 순위가 낮아서 Top-15에 안 잡힌다" → **Reranker 도입이 1순위**.

##### Phase 1: Reranker 최적화

Cross-encoder Reranker(bge-reranker-v2-m3)를 도입하여, 후보 풀(pool) 크기별 성능을 비교:

| 설정 | KW Accuracy | Context Recall | Faithfulness | Latency |
|:---|:---:|:---:|:---:|:---:|
| Baseline (Rerank 없음) | 0.625 | 0.633 | 0.825 | 329초 |
| Rerank pool=30→top15 | 0.616 | 0.748 | 0.806 | 373초 |
| **Rerank pool=50→top15** | **0.719** | **0.900** | **0.922** | **428초** |
| Rerank pool=100→top15 | 0.704 | 0.933 | 0.959 | 1,250초 |
| Diversity sampling | 0.692 | 0.900 | 0.922 | 1,161초 |

**핵심 발견**:
- Pool=30→50으로 넓히면 **KW Acc +0.103, CR +0.152** 대폭 개선
- Pool=50→100은 미미한 개선이지만 **Latency가 3배 증가** (428초 → 1,250초)
- Diversity sampling은 일반 Rerank보다 효과 없음
- **pool=50이 성능-지연 트레이드오프 최적점**

##### Phase 2: Embedding 모델 비교

3가지 Embedding 모델과 3가지 Alpha 값을 조합 (총 9개 설정):

| Embedding 모델 | α=0.3 (BM25 중심) | α=0.5 (균형) | α=0.7 (Vector 중심) |
|:---|:---:|:---:|:---:|
| **OpenAI** (text-embedding-3) | KW 0.651 / CR 0.850 | KW 0.714 / CR 0.900 | **KW 0.742 / CR 0.833** |
| **E5-Large** (multilingual) | KW 0.683 / CR 0.900 | KW 0.693 / CR 0.900 | KW 0.673 / CR 0.967 |
| **KURE-v1** (한국어 특화) | KW 0.713 / CR 0.967 | KW 0.682 / CR 0.967 | KW 0.732 / CR 0.967 |

**핵심 발견**:
- **KW Accuracy 기준**: OpenAI α=0.7이 0.742로 가장 높음
- **Context Recall 기준**: KURE-v1이 α에 상관없이 0.967로 가장 높음
- KURE-v1은 한국어에 특화되어 CR이 높지만, 최종 추출 정확도(KW Acc)에서는 OpenAI가 우세
- **Alpha=0.7(Vector 중심)이 OpenAI에서 최적** → Reranker가 있으면 Vector 비중을 높여도 됨

##### Phase 3: BM25 토크나이저

공백 분할(현행) 대비 형태소 분석기(Kiwi/Okt) 테스트 예정이었으나, 시간 제약으로 공백 분할만 측정:
- BM25 단독 Recall: 0.671
- Hybrid Recall: 0.706
- Rerank 후 Recall: 0.828

> 형태소 분석기 적용은 다음 실험 사이클에서 진행 예정.

### 실험 결과 종합

#### 핵심 지표 변화 추이 (Best Config 기준)

```
                  Context Recall    KW Accuracy    Faithfulness
                  ─────────────    ───────────    ────────────
Exp-01 (02/06)       0.733            -              -
Exp-02 (02/09)       0.767            -              -
Exp-03 (02/09)       0.733           0.492          0.963
Exp-04 (02/10)       0.700           0.664          0.829
Exp-04v2 (02/11)     0.733           0.663          0.817
Exp-04v3 (02/11)     0.900           0.742          0.922
                     ─────           ─────          ─────
총 개선폭            +0.167          +0.250         -0.041*
```

> *Faithfulness는 Exp-03에서 0.963이었으나, 그 때는 KW Accuracy가 0.492로 매우 낮았음.
> Exp-04v3에서는 KW Accuracy를 0.742까지 올리면서도 Faithfulness 0.922를 유지한 것이 핵심 성과.

#### 컴포넌트별 영향도 (무엇이 가장 효과적이었나?)

```
1순위: Reranker (Cross-encoder)     → KW_Acc +0.094, CR +0.267   [가장 큰 영향]
2순위: Verbatim Prompt              → KW_Acc +0.221              [추출 정확도 대폭 개선]
3순위: Embedding α 튜닝             → KW_Acc +0.022              [미세 조정]
─────
교훈:  아키텍처 복잡화 (Dual-Index, Query Decomposition 등) → 효과 없거나 역효과
```

#### 현재 최적 설정 (Current Best Config)

| 컴포넌트 | 설정값 | 확정 시점 |
|:---|:---|:---|
| Chunking | chunk_size=500, table=layout (표 보존) | Exp-01 |
| Retrieval | Hybrid (BM25+Vector), top_k=15 | Exp-02 |
| Reranker | bge-reranker-v2-m3, pool=50→top-15 | Exp-04v3 P1 |
| Embedding | OpenAI text-embedding-3, α=0.7 | Exp-04v3 P2 |
| Prompt | Verbatim + Fact Sheet (한국어) | Exp-04 |
| LLM | gpt-5-mini | 초기 설정 |

**현재 최종 성능**:
- **Keyword Accuracy: 0.742** (초기 0.492 대비 **+50.8% 개선**)
- **Context Recall: 0.900** (초기 0.733 대비 **+22.8% 개선**)
- **Faithfulness: 0.922** (목표 0.9 **달성**)

---

## 5. 남은 과제

### 5.1 RAG 최적화 (다음 실험)

| 실험 | 내용 | 기대 효과 | 우선순위 |
|:---|:---|:---|:---:|
| **Exp-05** | 일반화 검증 - 다른 도메인 RFP(건설, 정부R&D)로 테스트 | Best Config의 범용성 확인 | **높음** |
| **Exp-06** | HWP 포맷 확장 - HWP 문서에서도 동일 파이프라인 동작 확인 | 지원 포맷 확대 | 중간 |
| BM25 토크나이저 | 형태소 분석기(Kiwi/Okt) 적용 | 한국어 키워드 검색 정확도 개선 | 중간 |
| Table Integrity | 표 무결성 점수 기반 GRAY 게이팅 정교화 | 표 파싱 오류 시 안전 처리 | 중간 |

### 5.2 Enterprise 확장 로드맵

현재 BidFlow는 **단일 사용자 로컬 도구**입니다. 이를 **다중 사용자 사내 서비스**로 확장하기 위한 로드맵:

**Phase 4-1: 인증 시스템 (2주)**
- 현재: 누구나 접근 가능 (인증 없음)
- 목표: Streamlit-Authenticator로 로그인 구현, 세션 관리
- 효과: 최소한의 접근 제어 확보

**Phase 4-2: 사용자 데이터 격리 (2주)**
- 현재: 모든 분석 결과가 하나의 폴더에 저장
- 목표: user_id 기반 개인 저장소 분리 + "내 문서함" UI
- 효과: 사용자간 데이터 분리, 팀 공유 기능 기반 마련

**Phase 4-3: DB & API 분리 (4주+)**
- 현재: Streamlit 내부에 비즈니스 로직이 혼재
- 목표: FastAPI를 별도 서버로 분리, SQLite/PostgreSQL 도입
- 효과: 모바일 앱/사내 봇 등 다른 클라이언트 확장 가능

**추가 보안 강화**:
- Audit Log: 누가 언제 어떤 문서를 분석했는지 기록
- PII Masking: 업로드 시 개인정보 자동 마스킹
- 다중 프로필: "본사 기준", "지사 기준" 등 프로필을 여러 개 관리

> 상세 내용: `docs/planning/enterprise_roadmap.md`

### 5.3 보안 테스트

| 단계 | 테스트 항목 | 도구 | 목표 |
|:---|:---|:---|:---|
| Phase 1 | 프롬프트 인젝션(Jailbreak) | Garak (light) | Critical Fail 0건 |
| Phase 2 | PII 유출, 환각 유도, DoS 공격 | Garak (full) + 수동 Red Teaming | 공격 성공률 5% 미만 |

---

## 부록: 실험 환경

| 항목 | 값 |
|:---|:---|
| Golden Testset | 30문항, 12개 카테고리 |
| 대상 문서 | 고려대학교 차세대 포털·학사 정보시스템 구축사업 RFP |
| 생성 LLM | gpt-5-mini |
| 유틸리티 LLM | gpt-4o-mini |
| Embedding | OpenAI text-embedding-3 |
| Reranker | BAAI/bge-reranker-v2-m3 |
| Vector DB | ChromaDB |
| 평가 프레임워크 | RAGAS |
| 재현성 | seed=42 고정, 결과 JSON 버전 관리 |

---

*이 보고서의 모든 수치는 `bidflow/data/experiments/` 디렉토리의 JSON 파일에서 추출했습니다.*
*시각화 버전은 `bidflow/notebooks/intermediate_report.ipynb`에서 확인할 수 있습니다.*
