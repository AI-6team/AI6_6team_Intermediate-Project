# BidFlow 중간 발표 보고서

**프로젝트명**: BidFlow - 보안 강화형 지능형 입찰 분석 시스템
**슬로건**: *Don't just Write, Find & Verify.*
**발표일**: 2026-02-11
**팀**: AI6기 6팀

---

## 목차

1. [프로젝트 소개](#1-프로젝트-소개)
2. [시스템 설계](#2-시스템-설계)
3. [개발 방식 및 시나리오 안내](#3-개발-방식-및-시나리오-안내)
4. [시나리오 A: 다문서 실전 검증](#4-시나리오-a-다문서-실전-검증)
5. [시나리오 B: 단일 문서 심층 최적화](#5-시나리오-b-단일-문서-심층-최적화)
6. [시나리오 비교 및 종합 인사이트](#6-시나리오-비교-및-종합-인사이트)
7. [팀 공통 로드맵: Enterprise 확장과 보안](#7-팀-공통-로드맵-enterprise-확장과-보안)
8. [피드백 요청 사항](#8-피드백-요청-사항)
9. [부록: 용어 해설](#9-부록-용어-해설)

---

## 1. 프로젝트 소개

### 1.1 배경: 어떤 문제를 풀려고 하는가?

입찰 컨설턴트는 매일 수백 건의 공고와 수십 페이지의 **RFP(제안요청서)**를 검토합니다. 이 과정에서 가장 위험한 실수는 **필수 자격 요건을 놓치는 것**입니다.

- "보안 인증이 필요했는데 못 봤다"
- "실적 기준이 5건이었는데 3건으로 잘못 읽었다"
- "예산이 112.7억인데 113억으로 잘못 기재했다"

이런 실수는 입찰 부적격으로 직결됩니다. 기존 LLM 기반 도구들은 "그럴듯한 요약"은 잘 하지만, **정확한 수치 추출과 근거 검증에** 취약합니다.

### 1.2 목표

BidFlow는 **RAG(검색 증강 생성)** 기술을 활용하여:

1. RFP 문서에서 **필수/결격/독소 조항 및 핵심 항목을 구조적으로 추출**하고
2. **회사 프로필과의 비교 판정을 룰 기반으로 재현 가능하게 수행**하며
3. **근거 부족 시 GRAY(판단 유보)로 안전하게 처리**하는

**보안 우선(Security-First) 입찰 분석 시스템**을 구축합니다.

> **RAG란?** 외부 문서에서 관련 정보를 검색한 뒤, 그 정보를 기반으로 LLM(대규모 언어 모델)이 답변을 생성하는 기술입니다. LLM이 학습하지 않은 내부 문서의 정보도 정확하게 답변할 수 있게 해줍니다.

### 1.3 핵심 설계 원칙

우리 시스템의 가장 중요한 철학은 **"모르면 모른다고 말하기"**입니다.

| 원칙 | 설명 |
|:---|:---|
| **근거 강제** | 모든 답변에 반드시 출처(페이지, 표 셀 좌표)를 붙입니다. 근거 없는 답변은 출력하지 않습니다. |
| **단정 금지** | 근거가 불충분하면 "적격(GREEN)" 또는 "부적격(RED)"이라 단정 짓지 않고 **"판단 유보(GRAY)"**로 처리합니다. |
| **판정 분리** | LLM은 정보 추출만 하고, 최종 Go/No-Go 판정은 **룰 기반 Validator**가 합니다. LLM의 "판단"에 의존하지 않습니다. |
| **보안 우선** | 프롬프트 인젝션 공격 방어, 민감정보 마스킹 등 OWASP 보안 기준을 준수합니다. |

> **OWASP란?** Open Worldwide Application Security Project의 약자로, 웹 애플리케이션 보안 취약점을 연구하고 가이드라인을 제공하는 국제 비영리 보안 커뮤니티입니다. **OWASP Top 10 for LLM Applications**는 LLM 기반 시스템에서 가장 흔하고 위험한 10가지 보안 취약점(프롬프트 인젝션, 민감정보 유출, 학습 데이터 오염, 환각 등)을 정의하며, BidFlow는 이 기준을 준수하여 보안을 설계합니다.

### 1.4 요구사항 요약

**기능 요구사항 (FR)** - 시스템이 해야 할 일:

| ID | 기능 | 설명 |
|:---:|:---|:---|
| FR-01 | 문서 업로드 | RFP 파일(PDF, HWP) 업로드 및 버전 관리 |
| FR-02 | 문서 파싱 | 텍스트, 표, 메타데이터를 레이아웃 보존하며 파싱 |
| FR-03 | 핵심 항목 추출 | 30개 필수 항목(Compliance Matrix)을 그룹별 순차 추출 |
| FR-04 | Go/No-Go 판정 | 회사 프로필과 추출 항목을 룰 기반으로 비교/판정 |
| FR-05 | 결과 시각화 | Streamlit 기반 UI에서 근거 하이라이트와 함께 결과 표시 |
| FR-06 | 보안 | 3-Rail 아키텍처로 입력/처리/출력 전 구간 보안 적용 |

> FR = Functional Requirement (기능 요구사항). 시스템이 "무엇을 해야 하는지"를 정의합니다.

**30개 필수 항목(Compliance Matrix)이란?** RFP에서 반드시 확인해야 하는 핵심 정보를 4개 그룹으로 구조화한 체크리스트입니다. 의존성을 고려해 G1→G2→G3→G4 순서로 순차 추출합니다:

| 그룹 | 추출 항목 (예시) |
|:---|:---|
| **G1 기본정보** | 사업명, 발주기관, 사업유형, 총 사업비(예산), 계약방식, 사업 범위 등 |
| **G2 일정/제출** | 제안서 제출 마감일, 사업 기간, 제출 서류 목록, 제출 방식/형식 등 |
| **G3 자격/결격** | 참가 자격 요건, 필수 면허/인증, 실적 기준, 결격 사유, 제재 조건, 컨소시엄 허용 여부 등 |
| **G4 평가/배점** | 기술 평가 배점표, 가격 평가 방식, 평가 위원 구성, 우선 협상 기준, 보안 항목 배점 등 |

> G1에서 추출된 사업유형이 G3의 자격 요건 해석에 영향을 주고, G4의 배점표는 표(Table) 인덱스를 우선 검색하는 등 **그룹 간 의존성을 반영**합니다.

**비기능 요구사항 (NFR)** - 시스템의 품질 기준:

| ID | 기준 | 목표 |
|:---:|:---|:---|
| NFR-01 | 응답 시간 | 문서 파싱~추출 완료 30초 이내 |
| NFR-02 | 슬롯 누락률 | 필수 항목 누락률 10% 미만 |
| NFR-03 | 보안 테스트 | OWASP Top 10 LLM 취약점 Critical 0건 |
| NFR-04 | 환각 방지 | 근거 없는 단정 발생 시 GRAY 처리 |

> NFR = Non-Functional Requirement (비기능 요구사항). 시스템의 "품질/성능/보안 수준"을 정의합니다.

### 1.5 사용자 시나리오

```
1. 컨설턴트가 RFP 파일을 업로드한다
2. 시스템이 30개 필수 항목(Compliance Matrix)을 자동 추출한다
3. 각 항목에 근거(원문 위치)가 함께 표시된다
4. 컨설턴트가 회사 프로필을 선택/수정한다
5. 시스템이 Go/No-Go (GREEN/RED/GRAY) 판정을 내린다
6. 컨설턴트가 프로필을 수정하면, 재추출 없이 즉시 재판정이 된다
```

"프로필 수정 → 즉시 재판정"이 가능한 이유는 **추출 결과(불변)와 회사 프로필(가변)을 분리**했기 때문입니다. Validator는 두 입력을 받아 매번 새로 판정하므로, LLM을 다시 호출할 필요가 없습니다.

---

## 2. 시스템 설계

### 2.1 전체 아키텍처 (6단계 파이프라인)

```
[1] Ingest     →  [2] Parse      →  [3] Index       →  [4] Extract     →  [5] Validate    →  [6] Serve
(업로드/버전화)   (PDF/HWP 파싱)   (검색 인덱스 생성)   (핵심 항목 추출)   (룰 기반 판정)   (UI로 결과 표시)

                    ┌─────────────────────────────────────────────────────────┐
                    │  Security Layer: 3-Rail (Input Rail → Process Rail → Output Rail)  │
                    └─────────────────────────────────────────────────────────┘
```

| 단계 | 역할 | 핵심 포인트 |
|:---|:---|:---|
| **Ingest** | 파일 업로드, 문서 ID/해시 부여 | 동일 문서 재업로드 방지 |
| **Parse** | PDF/HWP에서 텍스트+표 추출 | **표 레이아웃 보존**이 핵심 (배점표, 일정표 등) |
| **Index** | 검색 가능한 인덱스 생성 | BM25(키워드) + Vector(의미) 이중 인덱스 |
| **Extract** | 30개 항목을 그룹별 순차 추출 | G1(기본) → G2(일정) → G3(자격) → G4(평가), 의존성 주입 |
| **Validate** | 회사 프로필과 비교하여 판정 | LLM 없이 룰 기반. GREEN/RED/GRAY 출력 |
| **Serve** | Streamlit UI로 결과 시각화 | 근거 하이라이트, 프로필 편집, 판정 뱃지 |

### 2.2 기술 스택

| 영역 | 기술 | 선택 이유 |
|:---|:---|:---|
| Backend | **FastAPI** | 비동기 고성능, Pydantic 자동 검증, API 문서 자동 생성 |
| Frontend | **Streamlit** | Python 기반 빠른 프로토타이핑, 데이터 시각화 용이 |
| LLM 연동 | **LangChain** | 다양한 LLM/도구 체인 연동 유연성 |
| Vector DB | **ChromaDB** / **FAISS** | 경량 벡터 저장소, 로컬 실행 용이 |
| 키워드 검색 | **BM25** | 특수 용어/숫자 검색에 강점 |
| 문서 파싱 | **pdfplumber** + **PyMuPDF** + **olefile** | 텍스트+표 하이브리드 파싱, HWP OLE 구조 분석 |
| 평가 | **RAGAS** | RAG 전용 평가 프레임워크 |

### 2.3 데이터 모델

모든 추출 결과는 **"값 + 상태 + 근거"** 구조를 따릅니다:

```
각 슬롯 = {
    value:      "112.7억원"                    ← 추출된 값
    status:     "FOUND | AMBIGUOUS | NOT_FOUND" ← 추출 상태
    evidence[]: [{page: 5, table: 2, row: 3}]   ← 근거 위치 (페이지/표/셀)
    integrity:  0.85                            ← 파싱 신뢰도 (0~1)
}
```

---

## 3. 개발 방식 및 시나리오 안내

### 3.1 개발 방식: Hive Mind 협업

우리 팀은 Anthropic(Claude 개발사)의 **Hive Mind** 업무 방식을 참고하여 개발을 진행했습니다.

> **Hive Mind란?** 고정된 역할 분담 대신, 각 팀원이 전체 문제를 독립적으로 풀어본 뒤 결과를 비교·통합하는 방식입니다. Anthropic에서는 이를 **"캠프파이어 방식"**이라고도 부르며, 프로젝트 주위로 인력이 유동적으로 모여 최적 결과를 도출합니다.

**우리 팀의 적용 방식**:

| Hive Mind 원칙 | 우리 팀 적용 |
|:---|:---|
| **캠프파이어 운영** | 팀장 기획서를 기반으로, 각 팀원이 독자적으로 전체 파이프라인을 구현 |
| **독립 실험 → 비교 통합** | 동일 문제를 다른 접근법(환경/모델/전략)으로 풀어 최적 전략 도출 |
| **AI 도구 내재화** | Claude Code 등 AI 도구를 개발 전 과정에서 적극 활용 |

이 방식의 핵심 장점은 **"어떤 전략이 통하고 어떤 전략이 안 통하는지"를 실증적으로 비교**할 수 있다는 점입니다. 아래 두 시나리오의 결과가 이를 증명합니다.

### 3.2 두 가지 시나리오

아래에서 두 가지 시나리오를 소개합니다. 각각 **다른 조건과 다른 접근법**으로 동일한 문제를 풀었습니다.

| 구분 | 시나리오 A | 시나리오 B |
|:---|:---|:---|
| **접근** | 다문서 실전 검증 | 단일 문서 심층 최적화 |
| **대상 문서** | 제공된 RFP 예제 전체 (다수) | 고려대학교 RFP 1건 |
| **실행 환경** | Linux (Ubuntu) | Windows 11 |
| **Vector DB** | Chroma | ChromaDB |
| **LLM** | GPT-4o | GPT-5-mini |
| **실험 방법** | Grid Search (파라미터 조합 탐색) | Phase-based 체계적 실험 (6회 반복) |
| **핵심 평가** | Recall (정답 문서 검색 성공률) | Context Recall, KW Accuracy, Faithfulness |
| **강점** | 다양한 문서 포맷에 대한 실전 검증 | 단일 문서에 대한 깊이 있는 최적화 |

> **왜 환경이 다른가?** 각자 독립적으로 개발했기 때문에 실행 환경도 달라졌습니다. 이러한 환경의 차이는 **"실제 배포 시 환경 호환성"에 대한 중요한 시사점**을 줍니다.

---

## 4. 시나리오 A: 다문서 실전 검증

### 4.1 접근 방식

시나리오 A에서는 제공된 **전체 RFP 예제 데이터**를 대상으로, **"총 사업비(예산)"** 같은 핵심 정보를 정확하게 추출하는 데 집중했습니다. 다수의 문서를 한꺼번에 처리하므로, 실전 환경에 가까운 테스트입니다.

**실험 환경**:
- OS: Linux (Ubuntu) - 운영 서버 환경 타겟
- 대상: `data/raw/files/` 폴더 내 전체 RFP 파일
- LLM: GPT-4o
- Reranker: BAAI/bge-reranker-v2-m3

### 4.2 HWP 파싱 실패부터 정답 삭제까지 — 3가지 함정

#### 문제 1: HWP/PDF 문서 파싱

| 문서 형식 | 문제 | 해결 |
|:---|:---|:---|
| **HWP** | 구형 파일에서 텍스트 깨짐, 표 데이터 누락, Windows 종속성 | `olefile`로 OLE 구조 직접 분석 + `zlib` 압축 해제 + UTF-16LE 디코딩하는 Custom Parser 구현 |
| **PDF** | 복잡한 레이아웃(다단, 표)에서 텍스트 순서 뒤섞임 | `pdfplumber`(표 정밀 파싱) + `PyMuPDF`(텍스트 속도) 하이브리드 방식 |

#### 문제 2: 검색 정확도 저조 (초기 61%)

"예산은 얼마인가?"라는 질문에 실제 금액이 적힌 문서 대신, "예산"이라는 단어가 많이 나오는 서식이나 일반 문서가 검색되는 문제가 발생했습니다.

**해결**: Hybrid Search 도입 (BM25 키워드 검색 + Vector 의미 검색 결합). "493,763,000원" 같은 구체적 수치는 BM25가 더 잘 찾아냅니다.

#### 문제 3: 필터링이 정답을 삭제

노이즈 제거를 위한 필터링 로직이 "별지 서식"에 포함된 진짜 예산표까지 삭제하는 부작용이 발생했습니다.

**해결**: **스마트 필터링** - 금액 패턴(정규식 `\d+(,\d{3})*원`)이 포함된 문서는 무조건 보존하도록 안전장치 추가.

### 4.3 Grid Search 9가지 조합 비교

다양한 파라미터 조합을 Grid Search로 탐색한 결과:

| Case | 설정 | 정확도 (Recall) | 소요 시간 | 비고 |
|:---|:---|:---:|:---:|:---|
| Case 1 | Vector Only, Chunk 1000 | 61.0% | 161초 | 기준점 (Baseline) |
| Case 2 | Hybrid (α=0.5) | 64.0% | 166초 | Hybrid 도입으로 소폭 상승 |
| Case 4 | Chunk 500 + K=6 | 70.0% | 167초 | 청크 크기 축소로 정밀도 향상 |
| **Case 5** | **Chunk 500 + K=10** | **73.0%** | **166초** | **최적: 속도와 정확도 균형** |
| Case 7 | Filtering + Rerank | 66.0% | 370초 | 필터링 과적용으로 정답 유실 |
| Case 9 | High Recall (K=50) + Rerank | 72.0% | 551초 | 정확도는 높으나 속도 3배 느림 |

> **Recall이란?** "정답 문서를 검색 결과에서 찾아온 비율"입니다. 73%면 100건 중 73건의 정답을 찾아왔다는 뜻입니다.

### 4.4 청크 500 + Hybrid가 정답이었다

```
1. 청크 크기 축소가 효과적  →  1000자 → 500자로 줄이면 정확도 +6%p
2. Hybrid Search가 필수    →  키워드(BM25) + 의미(Vector) 결합으로 +3%p
3. 필터링은 양날의 검       →  잘못 적용하면 정답까지 삭제 (과유불급)
4. Reranker는 조건부 유효  →  CPU 환경에서는 속도 대비 효과 미미
```

### 4.5 현재 최적 설정

```yaml
CHUNK_SIZE: 500
CHUNK_OVERLAP: 50
RETRIEVER_K: 10
USE_HYBRID: true
HYBRID_ALPHA: 0.5        # BM25 50% + Vector 50%
USE_RERANK: false         # CPU 환경에서 속도 이슈로 Off
```

**최종 성능**: 정확도(Recall) **73%** (Baseline 61% 대비 +12%p 개선)

### 4.6 향후 계획

| 우선순위 | 작업 | 설명 |
|:---:|:---|:---|
| 1 | **LLM 교체** | 현재 `GPT-4o` → 오픈소스 모델(`LLaMa` 또는 `Gemma`)로 전환하여 비용 절감 및 로컬 추론 환경 확보 |
| 2 | **테이블 파싱 고도화** | 복잡한 표 구조를 HTML/Markdown으로 변환하여 LLM의 표 이해도 향상 |
| 3 | **사용자 피드백 루프** | 실서비스에서 발생하는 오답 사례를 수집하여 테스트셋에 지속 추가 |
| 4 | **임베딩 모델 교체 검토** | 현재 `text-embedding-3-small` → Hugging Face 다국어 모델(`intfloat/multilingual-e5-large`, `BAAI/bge-m3`) 비교 테스트 |

---

## 5. 시나리오 B: 단일 문서 심층 최적화

### 5.1 접근 방식

시나리오 B에서는 **고려대학교 차세대 포털·학사 정보시스템 구축사업 RFP** 1건을 대상으로, **30개 필수 항목의 추출 정확도를 극한까지 끌어올리는** 데 집중했습니다. 사람이 직접 라벨링한 30문항의 **Golden Testset**을 구축하고, 체계적인 실험 프레임워크를 설계하여 단계별로 최적화를 진행했습니다.

**실험 환경**:
- OS: Windows 11
- 대상: 고려대학교 RFP 1건 + Golden Testset 30문항
- LLM: GPT-5-mini (생성), GPT-4o-mini (유틸리티)
- 평가: RAGAS 프레임워크
- 원칙: **한 번에 하나의 변수만 변경(One-lever Change)**, seed=42 고정

### 5.2 One-lever Change: 한 번에 하나만 바꾼다

한 번에 여러 변수를 바꾸면 **무엇 때문에** 성능이 향상되었는지 **파악하기 어려우므로**, 단계별로 최적 설정을 확정한 뒤 다음 단계로 넘어갑니다:

```
Phase A: Chunking 최적화     →  "문서를 어떻게 자르면 정보 손실이 적을까?"
Phase B: Retrieval 최적화    →  "검색 방식을 어떻게 조합하면 정답을 잘 찾을까?"
Phase C: Prompt 최적화       →  "LLM에게 어떻게 지시하면 정확하게 추출할까?"
Phase D: 통합 파이프라인      →  "위 결과를 종합하여 최적 조합을 찾자"
```

### 5.3 "검색은 되는데 추출이 안 된다" — 3가지 벽

#### 문제 1: "검색은 되는데 추출이 안 된다"

MVP 파이프라인이 동작은 하지만, **Context Recall(검색 품질) 0.733에 비해 Keyword Accuracy(추출 정확도)가 0.492로 매우 낮았습니다**. 문서에서 정보를 찾아오는 것까지는 괜찮은데, 찾아온 정보에서 정확한 값을 뽑아내지 못하는 상태.

> **Context Recall**: "필요한 정보를 얼마나 빠짐없이 검색했는가?" (높을수록 좋음)
> **Keyword Accuracy**: "추출한 답의 핵심 단어가 정답과 맞는가?" (높을수록 좋음)

#### 문제 2: "표(Table) 데이터 검색이 특히 안 된다"

EXP04-v3 Phase 0 진단에서 **표 기반 질문의 Recall이 텍스트보다 0.269만큼 낮았습니다** (Table: 0.524 vs Text: 0.792). 예산, 배점, 일정 등 입찰에서 가장 중요한 정보가 대부분 표에 있으므로 치명적인 문제.

#### 문제 3: "복잡하게 만들면 오히려 나빠진다"

Exp-04에서 Query Decomposition, Relevance Grading, Table Dual-Index 등 고급 기법을 도입했으나, **오히려 성능이 하락**하는 현상이 반복 발생. 아키텍처를 복잡하게 만들수록 Top-K 검색 결과에 노이즈가 유입되어 역효과.

**철학 전환**: "후보를 다양하게 만들자" → **"후보 품질을 직접 올리자"**

### 5.4 6차 실험: Chunking → Retrieval → Prompt → 통합

총 6차에 걸친 실험을 통해 점진적으로 성능을 개선했습니다:

#### Exp-01~03: 기초 최적화 (02/06~02/09)

| 실험 | 변경 변수 | 핵심 결과 | 교훈 |
|:---|:---|:---|:---|
| **Exp-01** (Chunking) | 청크 크기, 표 보존 방식 | chunk=500 + layout 보존 → CR 0.733 | **표가 깨지면 성능 급락** |
| **Exp-02** (Retrieval) | 검색 혼합 비율, Top-K | Hybrid α=0.5, K=15 → CR 0.767 | **Hybrid가 단독 검색보다 우수** |
| **Exp-03** (Prompt) | 프롬프트 언어/방식 | 한국어 Zero-shot → Faith 0.963 | **한국어 프롬프트가 환각 방지에 최적** |

> **Faithfulness**: "LLM이 지어낸 게 아니라 실제 문서에 있는 내용인가?" (높을수록 좋음, 환각 방지 핵심 지표)

#### Exp-04~04v2: 통합 및 시행착오 (02/10~02/11)

| 실험 | 시도 | 결과 | 교훈 |
|:---|:---|:---|:---|
| **Exp-04** | Verbatim Prompt, Metadata 주입, Query Decomposition 등 누적 적용 | Verbatim만 유효 (KW Acc +0.221), 나머지는 역효과 | **"원문 그대로 답하라"가 가장 강력** |
| **Exp-04v2** | Section-aware chunking, Table Dual-Index | 모든 변형이 Baseline보다 하락 | **구조 복잡화는 효과 없음** |

> **Verbatim Prompt**: "원문에 나온 표현 그대로 답변하라"는 지시. "112.7억원"을 "약 113억원"으로 바꾸는 것을 방지합니다.

#### Exp-04v3: 매칭 품질 직접 개선 (02/11, 현재 최신)

이전 실험들의 실패에서 배운 핵심: **구조를 복잡하게 만드는 것보다, 검색 결과의 품질 자체를 높이는 것이 정답**.

**Phase 0 (진단)**: 먼저 "어디가 문제인지" 정확히 파악

| 진단 항목 | 결과 | 의미 |
|:---|:---:|:---|
| Oracle Recall | 0.917 | 인덱스에 정답이 91.7% 존재 → 인덱싱은 문제 없음 |
| Recall@15 → @50 Gap | 0.115 | 순위만 올리면 11.5% 더 찾을 수 있음 → **Reranker 도입 결정** |
| Table vs Text Gap | -0.269 | 표 질문이 텍스트 질문보다 27% 낮음 → 표 검색이 병목 |

> **Oracle Recall**: "인덱스에 정답이 존재하기는 하는가?"의 이론적 상한. 이게 높으면 인덱싱은 잘 된 것이고, 검색 순위만 개선하면 됩니다.

**Phase 1 (Reranker)**: Cross-encoder 모델로 검색 결과 재순위

| 설정 | KW Accuracy | Context Recall | Faithfulness | Latency |
|:---|:---:|:---:|:---:|:---:|
| Baseline (Rerank 없음) | 0.625 | 0.633 | 0.825 | 329초 |
| Rerank pool=30→top15 | 0.616 | 0.748 | 0.806 | 373초 |
| **Rerank pool=50→top15** | **0.719** | **0.900** | **0.922** | **428초** |
| Rerank pool=100→top15 | 0.704 | 0.933 | 0.959 | 1,250초 |

> **Reranker란?** 1차 검색에서 넓게 50개를 가져온 뒤, 더 정교한 모델(Cross-encoder)로 다시 순위를 매겨 상위 15개만 사용하는 방식. "대략 찾고 → 정밀하게 골라내기" 전략입니다.

핵심: pool=50이 **성능-지연 트레이드오프의 최적점**. pool=100은 성능 향상은 미미한데 시간은 3배.

**Phase 2 (Embedding)**: 3가지 임베딩 모델 x 3가지 Alpha 값 비교

| Embedding 모델 | α=0.3 (BM25 중심) | α=0.5 (균형) | α=0.7 (Vector 중심) |
|:---|:---:|:---:|:---:|
| OpenAI | KW 0.651 | KW 0.714 | **KW 0.742** |
| E5-Large (다국어) | KW 0.683 | KW 0.693 | KW 0.673 |
| KURE-v1 (한국어 특화) | KW 0.713 | KW 0.682 | KW 0.732 |

> **Alpha(α)**: BM25와 Vector 검색의 혼합 비율. α=0이면 키워드만, α=1이면 의미 검색만 사용.

핵심: **OpenAI α=0.7**이 KW Accuracy 기준 최고. Reranker가 있으면 Vector 비중을 높여도 괜찮다는 발견.

### 5.5 핵심 발견: 무엇이 가장 효과적이었나?

```
1순위: Reranker (Cross-encoder)     → KW Acc +0.094, CR +0.267   [가장 큰 영향]
2순위: Verbatim Prompt              → KW Acc +0.221              [추출 정확도 대폭 개선]
3순위: Embedding Alpha 튜닝         → KW Acc +0.022              [미세 조정]
─────
교훈:  아키텍처 복잡화 (Dual-Index, Query Decomposition 등) → 효과 없거나 역효과
```

### 5.6 현재 최종 성능

| 지표 | 초기 (Exp-03) | 최종 (Exp-04v3) | 개선폭 |
|:---|:---:|:---:|:---:|
| **Keyword Accuracy** | 0.492 | **0.742** | **+50.8%** |
| **Context Recall** | 0.733 | **0.900** | **+22.8%** |
| **Faithfulness** | 0.963 | **0.922** | -4.3%* |

> *Faithfulness가 소폭 하락했지만, 초기에는 KW Accuracy가 0.492로 매우 낮았습니다. KW Accuracy를 0.742까지 끌어올리면서도 Faithfulness 0.922(목표 0.9 이상)를 유지한 것이 핵심 성과입니다.

### 5.7 향후 계획: EXP05 (Targeted Quality Enhancement)

다음 실험은 이미 설계를 마치고 노트북(`exp05_targeted_quality.ipynb`)까지 준비된 상태입니다. EXP04-v3에서 남은 실패 질문들을 **정밀 진단한 뒤, 원인별로 맞춤 개선**하는 것이 목표입니다.

**목표 지표**:

| 지표 | 현재 (EXP04-v3) | 목표 (EXP05) |
|:---|:---:|:---:|
| Keyword Accuracy | 0.742 | **≥ 0.80** |
| Context Recall | 0.900 | **≥ 0.93** |
| Faithfulness | 0.922 | **≥ 0.95** |

**실험 구조**:

```
Phase 0: 실패 질문 포렌식   →  실패 원인을 4가지로 분류 (INDEX/RETRIEVAL/RERANK/GENERATION)
Phase 1: 프롬프트 최적화    →  추출 강화 지시 / Evidence-first 출력 형식 비교
Phase 2: Reranker 동적 컷   →  score 급락점에서 자르는 Elbow 전략 검증
Phase 3: 통합 최적 설정     →  Phase 1~2 최적 조합 확정
```

**핵심 가설**:
1. 프롬프트 정밀화로 표/숫자 추출 정확도를 올리면 KW Accuracy와 Faithfulness를 동시에 개선할 수 있다
2. Reranker score 기반 동적 컷으로 현재 놓치고 있는 5개 질문을 복구하면 Context Recall이 올라간다
3. "근거를 먼저 **제시**한 뒤 값을 추출"하는 Evidence-first 형식이 Faithfulness 개선에 효과적이다

---

## 6. 시나리오 비교 및 종합 인사이트

### 6.1 두 시나리오에서 공통으로 발견한 것

두 시나리오는 서로 다른 조건(문서 수, 환경, LLM 모델)에서 진행했지만, **놀랍도록 일치하는 결론**이 나왔습니다:

| 발견 | 시나리오 A | 시나리오 B |
|:---|:---|:---|
| **청크 크기 500이 최적** | 1000→500으로 줄이니 +6%p | 500 > 1000 > 2000 순으로 성능 |
| **Hybrid Search 필수** | Vector Only 대비 +3%p | BM25+Vector 조합이 단독보다 우수 |
| **필터링/복잡화는 위험** | 필터링이 정답 삭제 | Query Decomposition이 역효과 |
| **Reranker 효과는 조건부** | CPU에서 속도 대비 효과 미미 | GPU에서 CR +0.267의 큰 효과 |

### 6.2 A는 실전 커버리지, B는 정밀 최적화

| 시나리오 A의 강점 | 시나리오 B의 강점 |
|:---|:---|
| 다양한 포맷(PDF/HWP/DOCX) 실전 검증 | 30문항 Golden Testset + RAGAS 정량 평가 |
| Linux 환경 호환성 확보 | 6차 반복 실험으로 최적 조합 도출 |
| 스마트 필터링(금액 패턴 보존) 발견 | Reranker + Verbatim Prompt 조합 발견 |
| 실사용 가까운 다문서 처리 | 표 vs 텍스트 도메인별 약점 진단 |

### 6.3 5가지 교훈: 복잡함보다 품질이 답이다

```
1. "단순하지만 각 구성 요소의 품질을 높이는 것"이 "복잡한 아키텍처"보다 효과적이다
2. 입찰 문서에서는 표(Table) 데이터 처리가 성능의 핵심 병목이다
3. BM25(키워드 검색)의 역할이 생각보다 크다 → 특수 용어/숫자가 많은 도메인 특성
4. Reranker는 GPU 환경에서만 실용적이다 → CPU에서는 속도 대비 효과 부족
5. "원문 그대로 답하라(Verbatim)"는 단순한 지시가 추출 정확도를 가장 크게 올린다
```

---

## 7. 팀 공통 로드맵: Enterprise 확장과 보안

각 시나리오의 RAG 최적화 계획은 위에서 개별적으로 다루었으므로, 여기서는 **팀 전체가 공통으로 진행할 과제**를 정리합니다.

### 7.1 로컬 도구 → 사내 서비스로 3단계 전환

현재 BidFlow는 **단일 사용자 로컬 도구**입니다. 이를 **다중 사용자 사내 서비스**로 확장하기 위한 로드맵은 다음과 같습니다:

| Phase | 기능 | 기간 | 핵심 내용 |
|:---:|:---|:---:|:---|
| **4-1** | 인증 시스템 | 2일 | 로그인 구현, 세션 관리 |
| **4-2** | 사용자 데이터 격리 | 2일 | user_id 기반 스토리지 분리, "내 문서함" |
| **4-3** | DB & API 분리 | **1주 이상** | FastAPI 서버 분리, DB 도입, 협업 기능 |

### 7.2 OWASP 기준 보안 테스트 2단계

| 단계 | 테스트 항목 | 도구 | 목표 |
|:---|:---|:---|:---|
| Phase 1 | 프롬프트 인젝션 방어 | Garak (light) | Critical Fail 0건 |
| Phase 2 | PII 유출, 환각 유도, DoS | Garak (full) + Red Teaming | 공격 성공률 5% 미만 |

---

## 8. 피드백 요청 사항

다음 사항들에 대해 피드백을 받고 싶습니다:

### 8.1 일반화 검증 대상 문서 선정

현재 모든 실험은 특정 RFP 문서 위주로 진행했습니다. Best Config가 다른 문서에서도 통하는지 확인이 필요한데, 어떤 도메인을 우선 테스트해야 할까요?

- 건설 분야 RFP (표 구조가 다를 수 있음)
- 정부 R&D 과제 RFP (규정 용어가 다를 수 있음)
- IT 서비스 RFP (현재와 유사한 도메인)

### 8.2 기능 확장 vs 성능 최적화 우선순위

- **옵션 A**: RAG 최적화(Exp-05, 06)를 먼저 완료한 뒤, Enterprise 확장
- **옵션 B**: 인증 시스템(Phase 4-1)을 먼저 적용하여 데모 가능한 형태를 만들고, RAG는 병렬 진행

### 8.3 표(Table) 도메인 성능 개선 방향

표 기반 질문의 검색 성능이 텍스트 대비 0.269만큼 낮습니다. Reranker로 어느 정도 보완되지만, 근본적인 대책이 필요할까요?

### 8.4 Reranker 적용 전략

시나리오 A에서는 CPU 환경에서 Reranker 효과가 미미했고, 시나리오 B에서는 GPU 환경에서 큰 효과가 있었습니다. 배포 환경에서 GPU 사용을 전제할 수 있을까요?

### 8.5 전반적인 접근 방식에 대한 의견

각자 독립 개발 후 통합하는 방식에 대한 피드백, 실험 설계 방법론에 대한 조언, 기획 방향에 대한 수정 의견 등이 있으면 부탁드립니다.

---

## 9. 부록: 용어 해설

### 일반 용어

| 용어 | 영문 | 설명 |
|:---|:---|:---|
| **RFP** | Request for Proposal | 제안요청서. 발주처가 입찰 참여자에게 제시하는 요구사항 문서 |
| **RAG** | Retrieval-Augmented Generation | 검색 증강 생성. 외부 문서에서 관련 정보를 검색한 뒤, 그 정보를 바탕으로 LLM이 답변을 생성하는 방식 |
| **LLM** | Large Language Model | 대규모 언어 모델. GPT 시리즈 등 자연어를 이해하고 생성하는 AI 모델 |
| **Compliance Matrix** | - | 적합성 매트릭스. RFP에서 추출해야 할 30개 필수 항목(슬롯)의 구조화된 체크리스트 |
| **Golden Testset** | - | 사람이 직접 정답을 라벨링한 평가용 데이터셋 |
| **GRAY** | - | 근거 부족 시 "판단 유보"로 안전하게 처리하는 상태 |

### 기획 용어

| 용어 | 영문 | 설명 |
|:---|:---|:---|
| **FR** | Functional Requirement | 기능 요구사항. 시스템이 "무엇을 해야 하는지" 정의 |
| **NFR** | Non-Functional Requirement | 비기능 요구사항. "품질/성능/보안 기준" 정의 |
| **MVP** | Minimum Viable Product | 최소 기능 제품. 핵심 기능만 갖춘 첫 번째 동작 버전 |

### 기술 용어

| 용어 | 설명 |
|:---|:---|
| **Chunking** | 긴 문서를 LLM이 처리 가능한 작은 단위(chunk)로 나누는 작업 |
| **Embedding** | 텍스트를 수치 벡터로 변환. 의미가 비슷한 텍스트는 비슷한 벡터값을 가짐 |
| **BM25** | 키워드 기반 검색 알고리즘. 특정 단어의 빈도/중요도로 점수를 매김 |
| **Hybrid Search** | BM25(키워드)와 Vector(의미) 검색을 결합한 방식 |
| **Alpha (α)** | Hybrid Search에서 두 검색의 혼합 비율 |
| **Reranker** | 1차 검색 결과를 더 정교한 모델로 다시 순위를 매기는 장치 |
| **Cross-encoder** | 질문과 문서를 함께 넣어 관련성을 직접 판단하는 Reranker 방식 |
| **Top-K** | 검색 결과에서 상위 K개만 사용하는 것 |
| **Verbatim Prompt** | "원문에 나온 표현 그대로 답하라"는 지시 |
| **Ablation Study** | 구성 요소를 하나씩 제거/추가하며 기여도를 측정하는 실험 방법 |
| **3-Rail** | Input/Process/Output 3단계 보안 아키텍처 |

### 평가 지표

| 지표 | 쉬운 설명 |
|:---|:---|
| **Context Recall** | "필요한 정보를 얼마나 빠짐없이 검색했는가?" (높을수록 좋음) |
| **Context Precision** | "검색 결과에 쓸모없는 것이 얼마나 섞여 있는가?" (높을수록 좋음) |
| **Keyword Accuracy** | "추출한 답의 핵심 단어가 정답과 맞는가?" (높을수록 좋음) |
| **Faithfulness** | "LLM이 지어낸 게 아니라 실제 문서 내용인가?" (환각 방지 지표) |
| **Recall** | "정답 문서를 검색 결과에서 찾아온 비율" (높을수록 좋음) |
| **MRR** | "정답이 검색 결과 상위에 있는가?" (높을수록 좋음) |
| **Oracle Recall** | "인덱스에 정답이 존재하는가?" (검색 성능의 이론적 상한) |
| **Latency** | "얼마나 빨리 결과가 나오는가?" (낮을수록 좋음) |

---

*상세 기획서: `AI6기_6팀_중급프로젝트_기획서.md`*
*시각화 노트북: `bidflow/notebooks/intermediate_report.ipynb`*
*실험 계획서: `bidflow/docs/planning/experiment_plan.md`*
*Enterprise 로드맵: `bidflow/docs/planning/enterprise_roadmap.md`*
