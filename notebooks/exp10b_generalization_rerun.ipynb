{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP10b: Multi-Document Generalization Rerun\n",
    "\n",
    "**Phase B** of HANDOFF_v2_next_experiments.md\n",
    "\n",
    "## 목표\n",
    "- 실제 운영 RAG 체인으로 A/B/C 설정 비교\n",
    "- dry-run proxy 제거, 실측 metrics 확보\n",
    "- 다문서 testset(Phase A 30문항)으로 일반화 성능 정량화\n",
    "\n",
    "## EXP09와의 차이점\n",
    "| 항목 | EXP09 (기존) | Phase B (신규) |\n",
    "|------|-------------|---------------|\n",
    "| 평가 파이프라인 | 실험용 자동 생성기 | **운영 RAG 체인** |\n",
    "| ops 지표 | dry-run proxy | **실측** (latency, timeout) |\n",
    "| 문서당 문항 수 | 1문항 | **6문항** |\n",
    "| 반복 횟수 | 1-run | **3-run** 평균 |\n",
    "| testset | 자동 생성 | **수동 검증된** golden_testset |\n",
    "| 평가 문서 | 100건 전체 | **5건 대표 문서** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 0: 환경 설정\n",
    "# ============================================================\n",
    "import os, sys, time, re, json, warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 프로젝트 루트 설정\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve()\n",
    "if PROJECT_ROOT.name == 'notebooks':\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# .env 로드\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "assert os.getenv('OPENAI_API_KEY'), 'OPENAI_API_KEY not found in .env'\n",
    "print('OpenAI API key loaded')\n",
    "\n",
    "# 실험 출력 디렉토리\n",
    "EXP_DIR = PROJECT_ROOT / 'data' / 'exp10b'\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Experiment output dir: {EXP_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: 문서 설정 및 Testset 로드\n",
    "# ============================================================\n",
    "\n",
    "# 5건 대표 문서 (Phase A에서 선정)\n",
    "DOC_CONFIGS = {\n",
    "    \"doc_A\": {\n",
    "        \"name\": \"수협중앙회 (text_only)\",\n",
    "        \"file_path\": \"data/raw/files/수협중앙회_수협중앙회 수산물사이버직매장 시스템 재구축 ISMP 수립 입.hwp\",\n",
    "        \"doc_type\": \"text_only\",\n",
    "        \"source_doc\": \"수협중앙회_수협중앙회 수산물사이버직매장 시스템 재구축 ISMP 수립 입.hwp\",\n",
    "    },\n",
    "    \"doc_B\": {\n",
    "        \"name\": \"한국교육과정평가원 (table_simple)\",\n",
    "        \"file_path\": \"data/raw/files/한국교육과정평가원_국가교육과정정보센터(NCIC) 시스템 운영 및 개선.hwp\",\n",
    "        \"doc_type\": \"table_simple\",\n",
    "        \"source_doc\": \"한국교육과정평가원_국가교육과정정보센터(NCIC) 시스템 운영 및 개선.hwp\",\n",
    "    },\n",
    "    \"doc_C\": {\n",
    "        \"name\": \"국립중앙의료원 (table_complex)\",\n",
    "        \"file_path\": \"data/raw/files/국립중앙의료원_(긴급)「2024년도 차세대 응급의료 상황관리시스템 구축.hwp\",\n",
    "        \"doc_type\": \"table_complex\",\n",
    "        \"source_doc\": \"국립중앙의료원_(긴급)「2024년도 차세대 응급의료 상황관리시스템 구축.hwp\",\n",
    "    },\n",
    "    \"doc_D\": {\n",
    "        \"name\": \"한국철도공사 (mixed)\",\n",
    "        \"file_path\": \"data/raw/files/한국철도공사 (용역)_예약발매시스템 개량 ISMP 용역.hwp\",\n",
    "        \"doc_type\": \"mixed\",\n",
    "        \"source_doc\": \"한국철도공사 (용역)_예약발매시스템 개량 ISMP 용역.hwp\",\n",
    "    },\n",
    "    \"doc_E\": {\n",
    "        \"name\": \"스포츠윤리센터 (hwp_representative)\",\n",
    "        \"file_path\": \"data/raw/files/재단법인스포츠윤리센터_스포츠윤리센터 LMS(학습지원시스템) 기능개선.hwp\",\n",
    "        \"doc_type\": \"hwp_representative\",\n",
    "        \"source_doc\": \"재단법인스포츠윤리센터_스포츠윤리센터 LMS(학습지원시스템) 기능개선.hwp\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# source_doc → doc_key 매핑 생성\n",
    "SOURCE_TO_KEY = {v[\"source_doc\"]: k for k, v in DOC_CONFIGS.items()}\n",
    "\n",
    "# Testset 로드\n",
    "testset = pd.read_csv('data/experiments/golden_testset_multi.csv')\n",
    "print(f\"Testset loaded: {len(testset)} questions\")\n",
    "print(f\"Documents: {testset['source_doc'].nunique()}\")\n",
    "print(f\"\\nQuestions per document:\")\n",
    "print(testset.groupby('source_doc').size())\n",
    "print(f\"\\nDifficulty distribution:\")\n",
    "print(testset['difficulty'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Helper Functions (kw_v2, 정규화 등)\n",
    "# ============================================================\n",
    "\n",
    "# 동의어 맵 (EXP06에서 검증된 버전)\n",
    "SYNONYM_MAP = {\n",
    "    '정보전략계획': 'ismp', 'ismp 수립': 'ismp', '정보화전략계획': 'ismp',\n",
    "    '통합로그인': 'sso', '단일 로그인': 'sso', '싱글사인온': 'sso',\n",
    "    '간편인증': '간편인증', '간편 인증': '간편인증',\n",
    "    '2차인증': '2차인증', '2차 인증': '2차인증', '추가인증': '2차인증',\n",
    "    'project manager': 'pm', '사업관리자': 'pm', '사업책임자': 'pm', '프로젝트 매니저': 'pm',\n",
    "    'project leader': 'pl', '프로젝트 리더': 'pl',\n",
    "    'quality assurance': 'qa', '품질관리': 'qa', '품질보증': 'qa',\n",
    "    '하자보수': '하자보수', '하자 보수': '하자보수',\n",
    "    '발주처': '발주기관', '발주 기관': '발주기관',\n",
    "}\n",
    "\n",
    "def normalize_answer_v2(text):\n",
    "    \"\"\"EXP06 검증된 정규화 v2\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text).strip().lower()\n",
    "    t = text.strip().lower()\n",
    "    # 1. 구두점 정리\n",
    "    t = re.sub(r'[\\u00b7\\u2027\\u2022\\u2219]', ' ', t)\n",
    "    t = re.sub(r'[\\u201c\\u201d\\u2018\\u2019\\u300c\\u300d\\u300e\\u300f]', '', t)\n",
    "    t = re.sub(r'[-\\u2013\\u2014]', ' ', t)\n",
    "    # 2. 숫자 콤마 제거\n",
    "    t = re.sub(r'(\\d),(?=\\d{3})', r'\\1', t)\n",
    "    # 3. 비율 통일\n",
    "    t = re.sub(r'(\\d+)\\s*(%|퍼센트|percent)', r'\\1%', t)\n",
    "    # 4. 통화 정리\n",
    "    t = re.sub(r'(\\d+)\\s*원', r'\\1원', t)\n",
    "    t = re.sub(r'(\\d+)\\s*억\\s*원', r'\\1억원', t)\n",
    "    t = re.sub(r'(\\d+)\\s*만\\s*원', r'\\1만원', t)\n",
    "    # 5. VAT 통일\n",
    "    t = t.replace('v.a.t', 'vat').replace('vat 포함', 'vat포함')\n",
    "    # 6. 동의어 치환\n",
    "    for orig, norm in SYNONYM_MAP.items():\n",
    "        t = t.replace(orig.lower(), norm)\n",
    "    # 7. 공백 정리\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def keyword_accuracy_v2(answer, ground_truth):\n",
    "    \"\"\"kw_v2: 정규화 후 키워드 매칭 정확도\"\"\"\n",
    "    ans_norm = normalize_answer_v2(answer)\n",
    "    gt_norm = normalize_answer_v2(ground_truth)\n",
    "    gt_words = [w for w in gt_norm.split() if len(w) > 1]\n",
    "    if not gt_words:\n",
    "        return 1.0\n",
    "    matched = sum(1 for w in gt_words if w in ans_norm)\n",
    "    return matched / len(gt_words)\n",
    "\n",
    "\n",
    "def compute_metrics_batch(results_df):\n",
    "    \"\"\"배치로 kw_v2 계산\"\"\"\n",
    "    kw_scores = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        score = keyword_accuracy_v2(row['answer'], row['ground_truth'])\n",
    "        kw_scores.append(score)\n",
    "    results_df['kw_v2'] = kw_scores\n",
    "    return results_df\n",
    "\n",
    "print('Helper functions loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: 5건 대표 문서 인덱싱 (Per-Document ChromaDB)\n",
    "# ============================================================\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from bidflow.parsing.hwp_parser import HWPParser\n",
    "from bidflow.parsing.preprocessor import TextPreprocessor\n",
    "\n",
    "# 실험 파라미터 (EXP04-v3 최적 설정)\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBEDDING_MODEL = 'text-embedding-3-small'\n",
    "\n",
    "parser = HWPParser()\n",
    "preprocessor = TextPreprocessor()\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=['\\n\\n', '\\n', ' ', '']\n",
    ")\n",
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "\n",
    "# Per-document ChromaDB 생성\n",
    "doc_vectordbs = {}  # doc_key -> Chroma instance\n",
    "doc_chunk_counts = {}\n",
    "\n",
    "for doc_key, doc_cfg in DOC_CONFIGS.items():\n",
    "    persist_dir = str(EXP_DIR / f'vectordb_{doc_key}')\n",
    "    file_path = str(PROJECT_ROOT / doc_cfg['file_path'])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Indexing {doc_key}: {doc_cfg['name']}\")\n",
    "    print(f\"File: {doc_cfg['file_path']}\")\n",
    "    \n",
    "    # 기존 인덱스가 있으면 재사용\n",
    "    if os.path.exists(persist_dir) and os.listdir(persist_dir):\n",
    "        print(f\"  -> Using existing index at {persist_dir}\")\n",
    "        vdb = Chroma(\n",
    "            persist_directory=persist_dir,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name='bidflow_rfp'\n",
    "        )\n",
    "        count = vdb._collection.count()\n",
    "        doc_vectordbs[doc_key] = vdb\n",
    "        doc_chunk_counts[doc_key] = count\n",
    "        print(f\"  -> Loaded {count} chunks\")\n",
    "        continue\n",
    "    \n",
    "    # 1. HWP 텍스트 추출 (production parser)\n",
    "    t0 = time.time()\n",
    "    raw_text = parser._parse_with_hwp5txt(file_path)\n",
    "    parse_method = 'hwp5txt'\n",
    "    if not raw_text:\n",
    "        raw_text = parser._parse_with_olefile(file_path)\n",
    "        parse_method = 'olefile'\n",
    "    parse_time = time.time() - t0\n",
    "    \n",
    "    if not raw_text:\n",
    "        print(f\"  !! FAILED to extract text from {doc_key}\")\n",
    "        continue\n",
    "    \n",
    "    # 2. 정규화\n",
    "    normalized_text = preprocessor.normalize(raw_text)\n",
    "    \n",
    "    # 3. 청킹 (실험 파라미터)\n",
    "    chunks = splitter.split_text(normalized_text)\n",
    "    \n",
    "    # 4. LangChain Document 생성\n",
    "    lc_docs = []\n",
    "    for i, chunk_text in enumerate(chunks):\n",
    "        lc_docs.append(Document(\n",
    "            page_content=chunk_text,\n",
    "            metadata={\n",
    "                'doc_key': doc_key,\n",
    "                'filename': doc_cfg['source_doc'],\n",
    "                'chunk_index': i,\n",
    "                'doc_type': doc_cfg['doc_type'],\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    # 5. ChromaDB에 인덱싱\n",
    "    t1 = time.time()\n",
    "    vdb = Chroma.from_documents(\n",
    "        documents=lc_docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name='bidflow_rfp'\n",
    "    )\n",
    "    index_time = time.time() - t1\n",
    "    \n",
    "    doc_vectordbs[doc_key] = vdb\n",
    "    doc_chunk_counts[doc_key] = len(chunks)\n",
    "    \n",
    "    print(f\"  -> Parse: {parse_method} ({parse_time:.1f}s), {len(raw_text):,} chars\")\n",
    "    print(f\"  -> Chunks: {len(chunks)} (size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})\")\n",
    "    print(f\"  -> Index: {index_time:.1f}s\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"\\nIndexing Summary:\")\n",
    "for k, v in doc_chunk_counts.items():\n",
    "    print(f\"  {k}: {v} chunks\")\n",
    "print(f\"  Total: {sum(doc_chunk_counts.values())} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: RAG 체인 팩토리 (Per-Document Pipeline)\n",
    "# ============================================================\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from typing import List, Any\n",
    "\n",
    "\n",
    "class ExperimentRetriever(BaseRetriever):\n",
    "    \"\"\"실험용 하이브리드 리트리버 (파라미터 완전 제어)\"\"\"\n",
    "    vector_retriever: Any = None\n",
    "    bm25_retriever: Any = None\n",
    "    weights: List[float] = [0.3, 0.7]\n",
    "    top_k: int = 15\n",
    "    pool_size: int = 50\n",
    "    use_rerank: bool = True\n",
    "    rerank_model: str = 'BAAI/bge-reranker-v2-m3'\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        search_k = self.pool_size if self.use_rerank else self.top_k\n",
    "\n",
    "        # BM25\n",
    "        try:\n",
    "            self.bm25_retriever.k = search_k * 2\n",
    "            bm25_docs = self.bm25_retriever.invoke(query)\n",
    "        except Exception:\n",
    "            bm25_docs = []\n",
    "\n",
    "        # Vector\n",
    "        try:\n",
    "            self.vector_retriever.search_kwargs['k'] = search_k * 2\n",
    "            vector_docs = self.vector_retriever.invoke(query)\n",
    "        except Exception:\n",
    "            vector_docs = []\n",
    "\n",
    "        # RRF Merge\n",
    "        rrf_top = self.pool_size if self.use_rerank else self.top_k\n",
    "        merged = self._rrf_merge(bm25_docs, vector_docs, k=60, limit=rrf_top)\n",
    "\n",
    "        # Rerank\n",
    "        if self.use_rerank and merged:\n",
    "            from bidflow.retrieval.rerank import rerank\n",
    "            merged = rerank(query, merged, top_k=self.top_k, model_name=self.rerank_model)\n",
    "\n",
    "        return merged\n",
    "\n",
    "    def _rrf_merge(self, list1, list2, k=60, limit=50):\n",
    "        scores = defaultdict(float)\n",
    "        doc_map = {}\n",
    "        w_bm25, w_vec = self.weights\n",
    "        for rank, doc in enumerate(list1):\n",
    "            scores[doc.page_content] += w_bm25 * (1 / (rank + k))\n",
    "            doc_map[doc.page_content] = doc\n",
    "        for rank, doc in enumerate(list2):\n",
    "            scores[doc.page_content] += w_vec * (1 / (rank + k))\n",
    "            if doc.page_content not in doc_map:\n",
    "                doc_map[doc.page_content] = doc\n",
    "        sorted_contents = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n",
    "        return [doc_map[c] for c in sorted_contents[:limit]]\n",
    "\n",
    "\n",
    "def build_retriever(vdb, alpha=0.7, top_k=15, pool_size=50, use_rerank=True):\n",
    "    \"\"\"ChromaDB 인스턴스로부터 하이브리드 리트리버 생성\"\"\"\n",
    "    vector_retriever = vdb.as_retriever(search_kwargs={'k': pool_size * 2})\n",
    "\n",
    "    # BM25 초기화 (ChromaDB에서 전체 문서 로드)\n",
    "    result = vdb.get()\n",
    "    all_docs = []\n",
    "    if result and result['documents']:\n",
    "        for i, text in enumerate(result['documents']):\n",
    "            meta = result['metadatas'][i] if result['metadatas'] else {}\n",
    "            all_docs.append(Document(page_content=text, metadata=meta))\n",
    "    bm25_retriever = BM25Retriever.from_documents(all_docs) if all_docs else BM25Retriever.from_documents([Document(page_content='empty')])\n",
    "    bm25_retriever.k = pool_size * 2\n",
    "\n",
    "    return ExperimentRetriever(\n",
    "        vector_retriever=vector_retriever,\n",
    "        bm25_retriever=bm25_retriever,\n",
    "        weights=[round(1 - alpha, 2), round(alpha, 2)],\n",
    "        top_k=top_k,\n",
    "        pool_size=pool_size,\n",
    "        use_rerank=use_rerank,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_rag_chain(retriever, model_name='gpt-5-mini'):\n",
    "    \"\"\"RAG 체인 생성 (운영 체인과 동일 프롬프트)\"\"\"\n",
    "    temp = 1 if model_name == 'gpt-5-mini' else 0\n",
    "    llm = ChatOpenAI(model=model_name, temperature=temp, timeout=60, max_retries=2)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        '아래 문맥(Context)만을 근거로 질문에 답하세요.\\n'\n",
    "        '반드시 원문에 있는 사업명, 기관명, 금액, 날짜 등의 표현을 그대로(Verbatim) 사용하세요.\\n'\n",
    "        '문맥에 답이 없으면 \\'해당 정보를 찾을 수 없습니다\\'라고 답하세요.\\n\\n'\n",
    "        '## 문맥 (Context)\\n{context}\\n\\n'\n",
    "        '## 질문\\n{question}\\n\\n'\n",
    "        '## 답변\\n'\n",
    "    )\n",
    "\n",
    "    def invoke_chain(question):\n",
    "        \"\"\"답변 생성 + 검색 컨텍스트 반환\"\"\"\n",
    "        t0 = time.time()\n",
    "        docs = retriever.invoke(question)\n",
    "        retrieval_time = time.time() - t0\n",
    "\n",
    "        context_text = '\\n\\n'.join([doc.page_content for doc in docs])\n",
    "\n",
    "        t1 = time.time()\n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        answer = chain.invoke({'context': context_text, 'question': question})\n",
    "        generation_time = time.time() - t1\n",
    "\n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'retrieved_contexts': [doc.page_content for doc in docs],\n",
    "            'n_retrieved': len(docs),\n",
    "            'retrieval_time': retrieval_time,\n",
    "            'generation_time': generation_time,\n",
    "            'total_time': retrieval_time + generation_time,\n",
    "        }\n",
    "\n",
    "    return invoke_chain\n",
    "\n",
    "print('RAG chain factory loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: 실험 설정 정의 (A/B/C Configs)\n",
    "# ============================================================\n",
    "\n",
    "# Config A: 현재 최적 설정 (EXP04-v3 best) - 모든 문서 동일\n",
    "CONFIG_A = {\n",
    "    'name': 'A_single_pipeline',\n",
    "    'description': '현재 최적 설정 그대로 (alpha=0.7, rerank, pool=50, top_k=15)',\n",
    "    'params': {\n",
    "        'alpha': 0.7,\n",
    "        'top_k': 15,\n",
    "        'pool_size': 50,\n",
    "        'use_rerank': True,\n",
    "    },\n",
    "    'doc_type_overrides': {},  # 문서 유형별 오버라이드 없음\n",
    "}\n",
    "\n",
    "# Config B: 문서 유형별 단일 라우팅\n",
    "CONFIG_B = {\n",
    "    'name': 'B_rule_single_route',\n",
    "    'description': '문서 유형별 alpha 조정 (text_only→0.5, 나머지→0.7)',\n",
    "    'params': {\n",
    "        'alpha': 0.7,  # 기본값\n",
    "        'top_k': 15,\n",
    "        'pool_size': 50,\n",
    "        'use_rerank': True,\n",
    "    },\n",
    "    'doc_type_overrides': {\n",
    "        'text_only': {'alpha': 0.5},  # BM25 비중 높임\n",
    "        'table_complex': {'alpha': 0.8, 'pool_size': 60},  # 벡터 비중 높임 + 풀 확대\n",
    "    },\n",
    "}\n",
    "\n",
    "# Config C: 보수적 확장 (wider net)\n",
    "CONFIG_C = {\n",
    "    'name': 'C_conservative_wide',\n",
    "    'description': '넓은 검색 범위 (top_k=20, pool=75) - multi-route 프록시',\n",
    "    'params': {\n",
    "        'alpha': 0.7,\n",
    "        'top_k': 20,\n",
    "        'pool_size': 75,\n",
    "        'use_rerank': True,\n",
    "    },\n",
    "    'doc_type_overrides': {},\n",
    "}\n",
    "\n",
    "ALL_CONFIGS = [CONFIG_A, CONFIG_B, CONFIG_C]\n",
    "N_RUNS = 3\n",
    "\n",
    "print('Experiment configs defined:')\n",
    "for cfg in ALL_CONFIGS:\n",
    "    print(f\"  {cfg['name']}: {cfg['description']}\")\n",
    "print(f\"\\nRuns per config: {N_RUNS}\")\n",
    "print(f\"Total evaluations: {len(ALL_CONFIGS)} configs × {N_RUNS} runs × {len(testset)} questions = {len(ALL_CONFIGS) * N_RUNS * len(testset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: RAG 실행 루프 (3 configs × 3 runs × 30 questions)\n",
    "# ============================================================\n",
    "\n",
    "all_results = []  # 전체 결과 저장\n",
    "errors = []  # 에러 로그\n",
    "\n",
    "total_evals = len(ALL_CONFIGS) * N_RUNS * len(testset)\n",
    "eval_count = 0\n",
    "exp_start = time.time()\n",
    "\n",
    "for config in ALL_CONFIGS:\n",
    "    config_name = config['name']\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# Config: {config_name}\")\n",
    "    print(f\"# {config['description']}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "\n",
    "    # Per-document 리트리버 생성 (config 파라미터 반영)\n",
    "    doc_chains = {}\n",
    "    for doc_key in DOC_CONFIGS:\n",
    "        doc_type = DOC_CONFIGS[doc_key]['doc_type']\n",
    "        params = dict(config['params'])  # 기본 파라미터 복사\n",
    "        # 문서 유형별 오버라이드 적용\n",
    "        if doc_type in config.get('doc_type_overrides', {}):\n",
    "            params.update(config['doc_type_overrides'][doc_type])\n",
    "\n",
    "        retriever = build_retriever(\n",
    "            doc_vectordbs[doc_key],\n",
    "            alpha=params['alpha'],\n",
    "            top_k=params['top_k'],\n",
    "            pool_size=params['pool_size'],\n",
    "            use_rerank=params['use_rerank'],\n",
    "        )\n",
    "        doc_chains[doc_key] = build_rag_chain(retriever)\n",
    "\n",
    "    for run_idx in range(N_RUNS):\n",
    "        run_start = time.time()\n",
    "        print(f\"\\n--- Run {run_idx + 1}/{N_RUNS} ---\")\n",
    "\n",
    "        for q_idx, row in testset.iterrows():\n",
    "            eval_count += 1\n",
    "            question = row['question']\n",
    "            ground_truth = row['ground_truth']\n",
    "            source_doc = row['source_doc']\n",
    "            doc_key = SOURCE_TO_KEY.get(source_doc)\n",
    "\n",
    "            if doc_key is None or doc_key not in doc_chains:\n",
    "                errors.append({'config': config_name, 'run': run_idx, 'question': question, 'error': f'No chain for {source_doc}'})\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = doc_chains[doc_key](question)\n",
    "                all_results.append({\n",
    "                    'config': config_name,\n",
    "                    'run': run_idx,\n",
    "                    'doc_key': doc_key,\n",
    "                    'doc_type': DOC_CONFIGS[doc_key]['doc_type'],\n",
    "                    'question': question,\n",
    "                    'ground_truth': ground_truth,\n",
    "                    'answer': result['answer'],\n",
    "                    'category': row.get('category', ''),\n",
    "                    'difficulty': row.get('difficulty', ''),\n",
    "                    'n_retrieved': result['n_retrieved'],\n",
    "                    'retrieval_time': result['retrieval_time'],\n",
    "                    'generation_time': result['generation_time'],\n",
    "                    'total_time': result['total_time'],\n",
    "                    'retrieved_contexts': result['retrieved_contexts'],\n",
    "                    'timeout': result['total_time'] > 120,\n",
    "                })\n",
    "                # Progress\n",
    "                if eval_count % 10 == 0:\n",
    "                    elapsed = time.time() - exp_start\n",
    "                    eta = (elapsed / eval_count) * (total_evals - eval_count)\n",
    "                    print(f\"  [{eval_count}/{total_evals}] elapsed={elapsed:.0f}s, ETA={eta:.0f}s\")\n",
    "\n",
    "            except Exception as e:\n",
    "                errors.append({'config': config_name, 'run': run_idx, 'question': question[:50], 'error': str(e)})\n",
    "                all_results.append({\n",
    "                    'config': config_name, 'run': run_idx, 'doc_key': doc_key,\n",
    "                    'doc_type': DOC_CONFIGS[doc_key]['doc_type'],\n",
    "                    'question': question, 'ground_truth': ground_truth,\n",
    "                    'answer': 'ERROR', 'category': row.get('category', ''),\n",
    "                    'difficulty': row.get('difficulty', ''),\n",
    "                    'n_retrieved': 0, 'retrieval_time': 0, 'generation_time': 0,\n",
    "                    'total_time': 0, 'retrieved_contexts': [], 'timeout': False,\n",
    "                })\n",
    "                print(f\"  ERROR: {question[:40]}... -> {e}\")\n",
    "\n",
    "        run_time = time.time() - run_start\n",
    "        print(f\"  Run {run_idx + 1} completed in {run_time:.0f}s\")\n",
    "\n",
    "total_time = time.time() - exp_start\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Total experiment time: {total_time:.0f}s ({total_time/60:.1f} min)\")\n",
    "print(f\"Total evaluations: {eval_count}\")\n",
    "print(f\"Errors: {len(errors)}\")\n",
    "\n",
    "# DataFrame 변환\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(f\"\\nResults DataFrame: {results_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: kw_v2 계산 및 기본 분석\n",
    "# ============================================================\n",
    "\n",
    "# kw_v2 계산\n",
    "results_df = compute_metrics_batch(results_df)\n",
    "\n",
    "# 중간 저장 (RAGAS 전)\n",
    "results_df.drop(columns=['retrieved_contexts']).to_csv(\n",
    "    str(EXP_DIR / 'exp10b_raw_results.csv'), index=False, encoding='utf-8-sig'\n",
    ")\n",
    "print('Raw results saved.\\n')\n",
    "\n",
    "# ── Config별 Overall Mean (3-run 평균) ──\n",
    "print('='*60)\n",
    "print('Config별 Overall Mean (3-run 평균)')\n",
    "print('='*60)\n",
    "\n",
    "config_summary = results_df.groupby('config').agg(\n",
    "    kw_v2_mean=('kw_v2', 'mean'),\n",
    "    kw_v2_std=('kw_v2', 'std'),\n",
    "    total_time_mean=('total_time', 'mean'),\n",
    "    total_time_p95=('total_time', lambda x: np.percentile(x, 95)),\n",
    "    timeout_rate=('timeout', 'mean'),\n",
    "    n_evals=('kw_v2', 'count'),\n",
    ").round(4)\n",
    "print(config_summary)\n",
    "\n",
    "# ── Config × Doc별 Macro Group Mean ──\n",
    "print('\\n' + '='*60)\n",
    "print('Config × Document별 kw_v2 (3-run 평균)')\n",
    "print('='*60)\n",
    "\n",
    "doc_pivot = results_df.groupby(['config', 'doc_key'])['kw_v2'].mean().unstack()\n",
    "doc_pivot['macro_mean'] = doc_pivot.mean(axis=1)  # Macro group mean\n",
    "print(doc_pivot.round(4))\n",
    "\n",
    "# ── Worst Group 분석 ──\n",
    "print('\\n' + '='*60)\n",
    "print('Config별 Worst Group (최저 성능 문서)')\n",
    "print('='*60)\n",
    "\n",
    "for cfg_name in results_df['config'].unique():\n",
    "    cfg_data = results_df[results_df['config'] == cfg_name]\n",
    "    doc_means = cfg_data.groupby('doc_key')['kw_v2'].mean()\n",
    "    worst = doc_means.idxmin()\n",
    "    print(f\"  {cfg_name}: worst={worst} (kw_v2={doc_means[worst]:.4f})\")\n",
    "\n",
    "# ── 난이도별 분석 ──\n",
    "print('\\n' + '='*60)\n",
    "print('Config × Difficulty별 kw_v2')\n",
    "print('='*60)\n",
    "\n",
    "diff_pivot = results_df.groupby(['config', 'difficulty'])['kw_v2'].mean().unstack()\n",
    "print(diff_pivot.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: RAGAS 평가 (faithfulness, context_recall)\n",
    "# ============================================================\n",
    "# 비용 절약: Config별 Run 0만 RAGAS 평가 (각 30문항 × 3 configs = 90 evals)\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, ContextRecall\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from bidflow.eval.ragas_runner import FixedTempChatOpenAI\n",
    "\n",
    "# RAGAS LLM/Embeddings 설정\n",
    "ragas_llm = LangchainLLMWrapper(\n",
    "    FixedTempChatOpenAI(model='gpt-5-mini', timeout=180, max_retries=3)\n",
    ")\n",
    "ragas_emb = LangchainEmbeddingsWrapper(\n",
    "    OpenAIEmbeddings(model='text-embedding-3-small')\n",
    ")\n",
    "\n",
    "ragas_results_list = []\n",
    "\n",
    "for config in ALL_CONFIGS:\n",
    "    config_name = config['name']\n",
    "    print(f\"\\nRAGAS evaluating: {config_name} (Run 0 only)\")\n",
    "\n",
    "    # Run 0 데이터만 추출\n",
    "    run0 = results_df[(results_df['config'] == config_name) & (results_df['run'] == 0)].copy()\n",
    "\n",
    "    if run0.empty:\n",
    "        print(f\"  No data for {config_name} run 0, skipping\")\n",
    "        continue\n",
    "\n",
    "    # RAGAS 데이터셋 구성\n",
    "    eval_dict = {\n",
    "        'user_input': run0['question'].tolist(),\n",
    "        'response': run0['answer'].tolist(),\n",
    "        'retrieved_contexts': run0['retrieved_contexts'].tolist(),\n",
    "        'reference': run0['ground_truth'].tolist(),\n",
    "    }\n",
    "    hf_dataset = Dataset.from_dict(eval_dict)\n",
    "\n",
    "    # 평가 실행\n",
    "    try:\n",
    "        ragas_result = evaluate(\n",
    "            dataset=hf_dataset,\n",
    "            metrics=[\n",
    "                Faithfulness(llm=ragas_llm),\n",
    "                ContextRecall(llm=ragas_llm),\n",
    "            ],\n",
    "            llm=ragas_llm,\n",
    "            embeddings=ragas_emb,\n",
    "            raise_exceptions=False,\n",
    "        )\n",
    "        ragas_df = ragas_result.to_pandas()\n",
    "        ragas_df['config'] = config_name\n",
    "        ragas_df['doc_key'] = run0['doc_key'].values\n",
    "        ragas_df['doc_type'] = run0['doc_type'].values\n",
    "        ragas_results_list.append(ragas_df)\n",
    "\n",
    "        print(f\"  Faithfulness: {ragas_df['faithfulness'].mean():.4f}\")\n",
    "        print(f\"  Context Recall: {ragas_df['context_recall'].mean():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  RAGAS evaluation failed: {e}\")\n",
    "\n",
    "if ragas_results_list:\n",
    "    ragas_all = pd.concat(ragas_results_list, ignore_index=True)\n",
    "    ragas_all.to_csv(str(EXP_DIR / 'exp10b_ragas_results.csv'), index=False, encoding='utf-8-sig')\n",
    "    print('\\nRAGAS results saved.')\n",
    "\n",
    "    # RAGAS 요약\n",
    "    print('\\n' + '='*60)\n",
    "    print('RAGAS Summary (Run 0)')\n",
    "    print('='*60)\n",
    "    ragas_summary = ragas_all.groupby('config')[['faithfulness', 'context_recall']].mean()\n",
    "    print(ragas_summary.round(4))\n",
    "else:\n",
    "    ragas_all = pd.DataFrame()\n",
    "    print('No RAGAS results generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 9: 종합 결과 집계 및 Quality Floor 판정\n",
    "# ============================================================\n",
    "\n",
    "# Quality Floor (HANDOFF v2 섹션 5.5)\n",
    "QUALITY_FLOOR = {\n",
    "    'kw_v2': 0.50,\n",
    "    'faithfulness': 0.80,\n",
    "    'context_recall': 0.60,\n",
    "}\n",
    "WORST_GROUP_FLOOR = {\n",
    "    'kw_v2': 0.35,\n",
    "    'faithfulness': 0.70,\n",
    "    'context_recall': 0.45,\n",
    "}\n",
    "OPS_CEILING = {\n",
    "    'timeout_rate': 0.10,\n",
    "    'p95_latency_sec': 120,\n",
    "}\n",
    "\n",
    "print('='*70)\n",
    "print('PHASE B: 종합 결과 보고')\n",
    "print('='*70)\n",
    "\n",
    "report = {}\n",
    "for config in ALL_CONFIGS:\n",
    "    config_name = config['name']\n",
    "    cfg_data = results_df[results_df['config'] == config_name]\n",
    "\n",
    "    # Overall metrics\n",
    "    overall_kw = cfg_data['kw_v2'].mean()\n",
    "    overall_kw_std = cfg_data['kw_v2'].std()\n",
    "    timeout_rate = cfg_data['timeout'].mean()\n",
    "    p95_latency = np.percentile(cfg_data['total_time'], 95)\n",
    "\n",
    "    # RAGAS (Run 0)\n",
    "    if not ragas_all.empty and config_name in ragas_all['config'].values:\n",
    "        ragas_cfg = ragas_all[ragas_all['config'] == config_name]\n",
    "        faith = ragas_cfg['faithfulness'].mean()\n",
    "        cr = ragas_cfg['context_recall'].mean()\n",
    "    else:\n",
    "        faith = np.nan\n",
    "        cr = np.nan\n",
    "\n",
    "    # Worst group\n",
    "    doc_means = cfg_data.groupby('doc_key')['kw_v2'].mean()\n",
    "    worst_doc = doc_means.idxmin()\n",
    "    worst_kw = doc_means.min()\n",
    "\n",
    "    # Macro group mean\n",
    "    macro_kw = doc_means.mean()\n",
    "\n",
    "    # Quality floor check\n",
    "    pass_kw = overall_kw >= QUALITY_FLOOR['kw_v2']\n",
    "    pass_faith = faith >= QUALITY_FLOOR['faithfulness'] if not np.isnan(faith) else False\n",
    "    pass_cr = cr >= QUALITY_FLOOR['context_recall'] if not np.isnan(cr) else False\n",
    "    pass_worst_kw = worst_kw >= WORST_GROUP_FLOOR['kw_v2']\n",
    "    pass_ops = timeout_rate <= OPS_CEILING['timeout_rate'] and p95_latency <= OPS_CEILING['p95_latency_sec']\n",
    "    all_pass = pass_kw and pass_faith and pass_cr and pass_worst_kw and pass_ops\n",
    "\n",
    "    report[config_name] = {\n",
    "        'kw_v2_overall': round(overall_kw, 4),\n",
    "        'kw_v2_std': round(overall_kw_std, 4),\n",
    "        'kw_v2_macro': round(macro_kw, 4),\n",
    "        'kw_v2_worst': round(worst_kw, 4),\n",
    "        'worst_doc': worst_doc,\n",
    "        'faithfulness': round(faith, 4) if not np.isnan(faith) else None,\n",
    "        'context_recall': round(cr, 4) if not np.isnan(cr) else None,\n",
    "        'timeout_rate': round(timeout_rate, 4),\n",
    "        'p95_latency_sec': round(p95_latency, 1),\n",
    "        'quality_floor_pass': all_pass,\n",
    "        'detail': {\n",
    "            'pass_kw': pass_kw,\n",
    "            'pass_faith': pass_faith,\n",
    "            'pass_cr': pass_cr,\n",
    "            'pass_worst_kw': pass_worst_kw,\n",
    "            'pass_ops': pass_ops,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    status = '✅ PASS' if all_pass else '❌ FAIL'\n",
    "    print(f\"\\n{config_name} {status}\")\n",
    "    print(f\"  kw_v2: {overall_kw:.4f} (±{overall_kw_std:.4f}) [floor={QUALITY_FLOOR['kw_v2']}] {'✅' if pass_kw else '❌'}\")\n",
    "    print(f\"  faithfulness: {faith:.4f} [floor={QUALITY_FLOOR['faithfulness']}] {'✅' if pass_faith else '❌'}\" if not np.isnan(faith) else f\"  faithfulness: N/A\")\n",
    "    print(f\"  context_recall: {cr:.4f} [floor={QUALITY_FLOOR['context_recall']}] {'✅' if pass_cr else '❌'}\" if not np.isnan(cr) else f\"  context_recall: N/A\")\n",
    "    print(f\"  worst_group: {worst_doc}={worst_kw:.4f} [floor={WORST_GROUP_FLOOR['kw_v2']}] {'✅' if pass_worst_kw else '❌'}\")\n",
    "    print(f\"  macro_group_mean: {macro_kw:.4f}\")\n",
    "    print(f\"  timeout_rate: {timeout_rate:.4f} [ceil={OPS_CEILING['timeout_rate']}]\")\n",
    "    print(f\"  p95_latency: {p95_latency:.1f}s [ceil={OPS_CEILING['p95_latency_sec']}s]\")\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('판정 요약')\n",
    "print('='*70)\n",
    "for name, r in report.items():\n",
    "    status = '✅ PASS' if r['quality_floor_pass'] else '❌ FAIL'\n",
    "    print(f\"  {name}: {status} (kw_v2={r['kw_v2_overall']}, faith={r['faithfulness']}, cr={r['context_recall']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: 결과 저장 (report.json + metrics.csv)\n",
    "# ============================================================\n",
    "\n",
    "# 1. Report JSON\n",
    "exp_report = {\n",
    "    'experiment': 'exp10b_generalization_rerun',\n",
    "    'phase': 'B',\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'testset': 'golden_testset_multi.csv',\n",
    "    'n_questions': len(testset),\n",
    "    'n_documents': len(DOC_CONFIGS),\n",
    "    'n_runs': N_RUNS,\n",
    "    'chunk_size': CHUNK_SIZE,\n",
    "    'chunk_overlap': CHUNK_OVERLAP,\n",
    "    'embedding_model': EMBEDDING_MODEL,\n",
    "    'llm_model': 'gpt-5-mini',\n",
    "    'doc_chunk_counts': doc_chunk_counts,\n",
    "    'configs': {c['name']: c for c in ALL_CONFIGS},\n",
    "    'quality_floor': QUALITY_FLOOR,\n",
    "    'worst_group_floor': WORST_GROUP_FLOOR,\n",
    "    'ops_ceiling': OPS_CEILING,\n",
    "    'results': report,\n",
    "    'total_evals': eval_count,\n",
    "    'total_errors': len(errors),\n",
    "    'errors': errors[:10],  # 최대 10개만 저장\n",
    "}\n",
    "\n",
    "report_path = 'data/experiments/exp10b_report.json'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(exp_report, f, ensure_ascii=False, indent=2, default=str)\n",
    "print(f'Report saved: {report_path}')\n",
    "\n",
    "# 2. Metrics CSV (retrieved_contexts 제외)\n",
    "metrics_path = 'data/experiments/exp10b_metrics.csv'\n",
    "results_df.drop(columns=['retrieved_contexts']).to_csv(\n",
    "    metrics_path, index=False, encoding='utf-8-sig'\n",
    ")\n",
    "print(f'Metrics saved: {metrics_path}')\n",
    "\n",
    "# 3. 에러 로그\n",
    "if errors:\n",
    "    error_path = str(EXP_DIR / 'exp10b_errors.json')\n",
    "    with open(error_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(errors, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Errors saved: {error_path}')\n",
    "\n",
    "print(f'\\n실험 완료! Phase B 결과 저장 완료.')\n",
    "print(f'다음 단계: HISTORY_v2_execution.md 업데이트')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: doc_type 별 text vs table 성능 비교 (Gap 분석)\n",
    "# ============================================================\n",
    "\n",
    "print('='*70)\n",
    "print('Document Type 별 성능 비교 (text vs table Gap)')\n",
    "print('='*70)\n",
    "\n",
    "# text_only vs table-heavy 비교\n",
    "for config_name in results_df['config'].unique():\n",
    "    cfg_data = results_df[results_df['config'] == config_name]\n",
    "\n",
    "    text_kw = cfg_data[cfg_data['doc_type'] == 'text_only']['kw_v2'].mean()\n",
    "    table_types = ['table_simple', 'table_complex', 'mixed', 'hwp_representative']\n",
    "    table_kw = cfg_data[cfg_data['doc_type'].isin(table_types)]['kw_v2'].mean()\n",
    "    gap = text_kw - table_kw\n",
    "\n",
    "    print(f\"\\n{config_name}:\")\n",
    "    print(f\"  text_only kw_v2: {text_kw:.4f}\")\n",
    "    print(f\"  table-docs kw_v2: {table_kw:.4f}\")\n",
    "    print(f\"  text-table gap: {gap:.4f} ({gap*100:.1f}%p)\")\n",
    "\n",
    "# Category별 분석\n",
    "print('\\n' + '='*70)\n",
    "print('Category별 kw_v2 (전체 config 평균)')\n",
    "print('='*70)\n",
    "\n",
    "cat_analysis = results_df.groupby('category')['kw_v2'].agg(['mean', 'std', 'count'])\n",
    "cat_analysis = cat_analysis.sort_values('mean', ascending=False)\n",
    "print(cat_analysis.round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}