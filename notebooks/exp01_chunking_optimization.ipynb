{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "337b133b",
      "metadata": {},
      "source": [
        "# Exp-01: Chunking Strategy Optimization\n",
        "- **Date**: 2026-02-05\n",
        "- **Goal**: RFP 문서에 최적화된 Chunk Size 및 Table Strategy 도출\n",
        "- **Focus**: Context Recall Maximization (Target: 0.85+)\n",
        "\n",
        "## 변경 이력\n",
        "- v2: Human-labeled Ground Truth 기반 실제 평가로 전환 (Mock Score 제거)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44d342d9",
      "metadata": {},
      "source": [
        "## 1. 실험 설계 (Experiment Design)\n",
        "### 1.1 변수 (Variables)\n",
        "- **Independent (실험군)**:\n",
        "  - **Chunk Size**: [500, 1000, 2000]\n",
        "  - **Table Strategy**: [`Text` (단순), `Layout` (구조 보존)]\n",
        "- **Controlled (통제 변수)**:\n",
        "  - Retrieval: Hybrid Search (Default)\n",
        "  - Top-K: 10\n",
        "\n",
        "### 1.2 가설 (Hypothesis)\n",
        "- RFP는 표(Table)에 중요한 스펙 정보가 많으므로, `Layout` 전략이 `Text`보다 Recall이 높을 것이다.\n",
        "- 문맥이 긴 문서 특성상 500자보다는 1000자가 유리하나, 2000자는 검색 노이즈가 발생할 것이다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec851ea",
      "metadata": {},
      "source": [
        "## 2. 환경 설정 (Setup)\n",
        "데이터 로더 및 필요 라이브러리를 임포트합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95c9975",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 프로젝트 루트 경로 추가\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# BidFlow 모듈\n",
        "from bidflow.ingest.loader import RFPLoader\n",
        "from bidflow.ingest.storage import DocumentStore, VectorStoreManager\n",
        "from bidflow.retrieval.hybrid_search import HybridRetriever\n",
        "from bidflow.eval.ragas_runner import RagasRunner\n",
        "from bidflow.parsing.pdf_parser import PDFParser\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['font.family'] = 'Malgun Gothic'  # 한글 폰트\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"[System] 모듈 로드 완료\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a99535",
      "metadata": {},
      "source": [
        "## 3. Baseline 설정\n",
        "- **Setting**: Chunk 500 / Text Strategy\n",
        "- **Target Metric**: Context Recall\n",
        "- **Budget**: Latency < 1.0s\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "896c8401",
      "metadata": {},
      "source": [
        "## 4. 데이터 및 파이프라인 (Data & Pipeline)\n",
        "- **Dataset**: 실제 RFP 문서 샘플 (`data/raw/files`)\n",
        "- **Pipeline**: `RFPLoader` -> `Chunking` -> `Ragas Eval` -> `Report`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7923f5ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 경로 설정\n",
        "DATA_DIR = \"../data/raw/files\"\n",
        "META_PATH = \"../data/raw/data_list.csv\"\n",
        "GOLDEN_TESTSET_PATH = \"../data/experiments/golden_testset.csv\"\n",
        "CHROMA_BASE_PATH = \"../data/chroma_exp\"  # 실험용 별도 DB\n",
        "\n",
        "# 실험 변수 정의\n",
        "CHUNK_SIZES = [500, 1000, 2000]\n",
        "TABLE_STRATEGIES = [\"text\", \"layout\"]\n",
        "\n",
        "# 1. Golden Testset 로드 (Human-labeled)\n",
        "print(\"=\" * 50)\n",
        "print(\"1. Golden Testset 로드\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if os.path.exists(GOLDEN_TESTSET_PATH):\n",
        "    golden_df = pd.read_csv(GOLDEN_TESTSET_PATH)\n",
        "    # 템플릿 데이터 필터링 (실제 레이블된 데이터만)\n",
        "    golden_df = golden_df[~golden_df['ground_truth'].str.contains(r'\\[여기에', na=False)]\n",
        "    \n",
        "    if len(golden_df) == 0:\n",
        "        print(\"[Warning] Golden Testset이 비어있습니다!\")\n",
        "        print(f\"   - {GOLDEN_TESTSET_PATH} 파일을 열어 실제 정답을 입력하세요.\")\n",
        "        print(\"   - question: 평가할 질문\")\n",
        "        print(\"   - ground_truth: 문서에서 찾을 수 있는 정답\")\n",
        "        TESTSET_READY = False\n",
        "    else:\n",
        "        print(f\"[OK] {len(golden_df)}개 테스트 케이스 로드됨\")\n",
        "        print(golden_df[['question', 'category']].head())\n",
        "        TESTSET_READY = True\n",
        "else:\n",
        "    print(f\"[Error] Golden Testset 파일이 없습니다: {GOLDEN_TESTSET_PATH}\")\n",
        "    TESTSET_READY = False\n",
        "\n",
        "# 2. 샘플 PDF 파일 찾기\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"2. 샘플 PDF 파일 검색\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "SAMPLE_FILE = None\n",
        "if os.path.exists(DATA_DIR):\n",
        "    pdfs = [f for f in os.listdir(DATA_DIR) if f.lower().endswith('.pdf')]\n",
        "    if pdfs:\n",
        "        SAMPLE_FILE = os.path.join(DATA_DIR, pdfs[0])\n",
        "        print(f\"[OK] 샘플 PDF: {pdfs[0]}\")\n",
        "    else:\n",
        "        print(\"[Error] PDF 파일을 찾을 수 없습니다.\")\n",
        "else:\n",
        "    print(f\"[Error] 데이터 디렉토리가 없습니다: {DATA_DIR}\")\n",
        "\n",
        "# 결과 저장용\n",
        "results = []\n",
        "chunk_stats = []  # 청크 통계"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "golden_testset_gen",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [Quick Fix] Reload RagasRunner explicitly to apply simplified transforms\n",
        "import importlib\n",
        "import bidflow.eval.ragas_runner\n",
        "importlib.reload(bidflow.eval.ragas_runner)\n",
        "from bidflow.eval.ragas_runner import RagasRunner\n",
        "runner = RagasRunner()\n",
        "\n",
        "TESTSET_PATH = \"../data/experiments/golden_testset.csv\"\n",
        "\n",
        "if os.path.exists(TESTSET_PATH):\n",
        "    print(\"Loading existing Golden Testset...\")\n",
        "    testset_df = pd.read_csv(TESTSET_PATH)\n",
        "else:\n",
        "    print(\"Generating Golden Testset (This may take a while)...\")\n",
        "    if SAMPLE_FILE:\n",
        "        # 1. Baseline Ingestion\n",
        "        loader.vec_manager.clear() # Clean State\n",
        "        with open(SAMPLE_FILE, \"rb\") as f:\n",
        "             # Default: chunk=1000, table=text (표준)\n",
        "             doc_hash = loader.process_file(f, os.path.basename(SAMPLE_FILE), chunk_size=1000, table_strategy=\"text\")\n",
        "        \n",
        "        # 2. Load Parsed Documents for Ragas\n",
        "        # Ragas needs LangChain Document list. We can reconstruct it from the loaded RFPDocument\n",
        "        rfp_doc = loader.doc_store.load_document(doc_hash)\n",
        "        lc_docs = []\n",
        "        from langchain_core.documents import Document\n",
        "        for chunk in rfp_doc.chunks:\n",
        "            lc_docs.append(Document(page_content=chunk.text, metadata=chunk.metadata))\n",
        "            \n",
        "        # 3. Generate\n",
        "        testset_df = runner.generate_testset(lc_docs, test_size=5) # 5 Questions\n",
        "        testset_df.to_csv(TESTSET_PATH, index=False)\n",
        "        print(f\"Golden Testset saved to {TESTSET_PATH}\")\n",
        "    else:\n",
        "        print(\"No Sample File found! Cannot generate testset.\")\n",
        "        testset_df = pd.DataFrame()\n",
        "\n",
        "display(testset_df.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c5879ff",
      "metadata": {},
      "source": [
        "## 5. 평가 지표 (Metrics)\n",
        "- **Context Recall**: 정답(Ground Truth)이 검색된 청크 내에 존재하는지 여부. (RFP 분석의 핵심)\n",
        "- **Latency (p95)**: 검색 응답 속도.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03491edd",
      "metadata": {},
      "source": [
        "## 6. 실험 실행 (Execution)\n",
        "정의된 변수 조합(Grid Search)에 따라 파이프라인을 실행합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b0ab2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_single_experiment(\n",
        "    pdf_path: str,\n",
        "    chunk_size: int,\n",
        "    table_strategy: str,\n",
        "    golden_df: pd.DataFrame,\n",
        "    exp_chroma_path: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    단일 설정에 대해 Chunking -> Indexing -> Retrieval 평가 수행\n",
        "    \"\"\"\n",
        "    # ===== Fix 1: 모든 필요한 임포트 추가 =====\n",
        "    import numpy as np\n",
        "    import gc\n",
        "    import time\n",
        "    import shutil\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    from langchain_chroma import Chroma\n",
        "    from langchain_core.documents import Document\n",
        "    \n",
        "    # ===== Fix 2: config_name 정의 (맨 앞에!) =====\n",
        "    config_name = f\"chunk={chunk_size}_table={table_strategy}\"\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"[Experiment] {config_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    def robust_rmtree(path):\n",
        "        \"\"\"Windows 파일 락 대응 삭제\"\"\"\n",
        "        if not os.path.exists(path): \n",
        "            return\n",
        "        for _ in range(5):\n",
        "            try:\n",
        "                shutil.rmtree(path)\n",
        "                return\n",
        "            except PermissionError:\n",
        "                gc.collect()\n",
        "                time.sleep(1)\n",
        "            except Exception as e:\n",
        "                print(f'Warning: rmtree failed: {e}')\n",
        "                return\n",
        "        print(f'Failed to delete {path} after retries.')\n",
        "    \n",
        "    # ===== Fix 3: 실험 DB 초기화 (robust_rmtree 호출) =====\n",
        "    robust_rmtree(exp_chroma_path)\n",
        "    os.makedirs(exp_chroma_path, exist_ok=True)\n",
        "    \n",
        "    # 1. PDF 파싱\n",
        "    start_time = time.perf_counter()\n",
        "    \n",
        "    parser = PDFParser()\n",
        "    chunks = parser.parse(\n",
        "        pdf_path,\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size * 0.1),\n",
        "        table_strategy=table_strategy\n",
        "    )\n",
        "    \n",
        "    parse_time = time.perf_counter() - start_time\n",
        "    print(f\"  파싱 완료: {len(chunks)}개 청크 ({parse_time:.2f}s)\")\n",
        "    \n",
        "    # 청크 통계\n",
        "    chunk_lengths = [len(c.text) for c in chunks]\n",
        "    chunk_stat = {\n",
        "        \"config\": config_name,\n",
        "        \"chunk_size\": chunk_size,\n",
        "        \"table_strategy\": table_strategy,\n",
        "        \"num_chunks\": len(chunks),\n",
        "        \"avg_length\": float(np.mean(chunk_lengths)),\n",
        "        \"std_length\": float(np.std(chunk_lengths)),\n",
        "        \"min_length\": int(np.min(chunk_lengths)),\n",
        "        \"max_length\": int(np.max(chunk_lengths)),\n",
        "    }\n",
        "    \n",
        "    # 2. VectorStore 구축\n",
        "    start_time = time.perf_counter()\n",
        "    \n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "    vector_db = Chroma(\n",
        "        persist_directory=exp_chroma_path,\n",
        "        embedding_function=embeddings,\n",
        "        collection_name=\"exp_chunking\"\n",
        "    )\n",
        "    \n",
        "    lc_docs = [\n",
        "        Document(\n",
        "            page_content=c.text,\n",
        "            metadata={\"chunk_id\": c.chunk_id, \"page\": c.page_no}\n",
        "        )\n",
        "        for c in chunks\n",
        "    ]\n",
        "    vector_db.add_documents(lc_docs)\n",
        "    \n",
        "    index_time = time.perf_counter() - start_time\n",
        "    print(f\"  인덱싱 완료 ({index_time:.2f}s)\")\n",
        "    \n",
        "    # 3. Retrieval 및 Context Recall 평가\n",
        "    start_time = time.perf_counter()\n",
        "    \n",
        "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 10})\n",
        "    \n",
        "    questions = []\n",
        "    ground_truths = []\n",
        "    contexts_list = []\n",
        "\n",
        "    for _, row in golden_df.iterrows():\n",
        "        question = row['question']\n",
        "        ground_truth = str(row['ground_truth'])\n",
        "        \n",
        "        retrieved_docs = retriever.invoke(question)\n",
        "        contexts_list.append([doc.page_content for doc in retrieved_docs])\n",
        "        questions.append(question)\n",
        "        ground_truths.append(ground_truth)\n",
        "\n",
        "    # ===== Fix 4: Ragas v0.2+ API 호환 키 사용 =====\n",
        "    from bidflow.eval.ragas_runner import RagasRunner\n",
        "    from ragas import evaluate\n",
        "    from ragas.metrics import ContextRecall\n",
        "    from datasets import Dataset\n",
        "\n",
        "    try:\n",
        "        # Ragas v0.2+ 키 이름: user_input, reference, retrieved_contexts\n",
        "        data = {\n",
        "            \"user_input\": questions,\n",
        "            \"reference\": ground_truths,\n",
        "            \"retrieved_contexts\": contexts_list\n",
        "        }\n",
        "        dataset = Dataset.from_dict(data)\n",
        "        runner = RagasRunner()\n",
        "        \n",
        "        print(f\"  Ragas Evaluating {len(dataset)} items...\")\n",
        "        eval_result = evaluate(\n",
        "            dataset,\n",
        "            metrics=[ContextRecall(llm=runner.llm)],\n",
        "            llm=runner.llm,\n",
        "            embeddings=runner.embeddings,\n",
        "            raise_exceptions=False\n",
        "        )\n",
        "        \n",
        "        raw_metric = eval_result[\"context_recall\"]\n",
        "        if isinstance(raw_metric, (list, tuple)):\n",
        "            avg_recall = float(np.mean(raw_metric))\n",
        "        elif hasattr(raw_metric, \"mean\"):\n",
        "            avg_recall = float(raw_metric.mean())\n",
        "        else:\n",
        "            avg_recall = float(raw_metric) if raw_metric is not None else 0.0\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  Ragas Error: {e}, falling back to 0\")\n",
        "        avg_recall = 0.0\n",
        "\n",
        "    retrieval_time = time.perf_counter() - start_time\n",
        "    print(f\"  검색 테스트 완료: Context Recall = {avg_recall:.4f} ({retrieval_time:.2f}s)\")\n",
        "    \n",
        "    # 4. 결과 반환\n",
        "    result = {\n",
        "        \"config\": config_name,\n",
        "        \"chunk_size\": chunk_size,\n",
        "        \"table_strategy\": table_strategy,\n",
        "        \"context_recall\": avg_recall,\n",
        "        \"num_chunks\": len(chunks),\n",
        "        \"parse_time\": float(parse_time),\n",
        "        \"index_time\": float(index_time),\n",
        "        \"retrieval_time\": float(retrieval_time),\n",
        "        \"latency_total\": float(parse_time + index_time + retrieval_time),\n",
        "    }\n",
        "    \n",
        "    return result, chunk_stat\n",
        "\n",
        "\n",
        "# 실험 실행\n",
        "if SAMPLE_FILE and TESTSET_READY:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"실험 시작: Grid Search\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for table_strat in TABLE_STRATEGIES:\n",
        "        for size in CHUNK_SIZES:\n",
        "            exp_db_path = os.path.join(CHROMA_BASE_PATH, f\"chunk{size}_{table_strat}\")\n",
        "            \n",
        "            try:\n",
        "                result, chunk_stat = run_single_experiment(\n",
        "                    pdf_path=SAMPLE_FILE,\n",
        "                    chunk_size=size,\n",
        "                    table_strategy=table_strat,\n",
        "                    golden_df=golden_df,\n",
        "                    exp_chroma_path=exp_db_path\n",
        "                )\n",
        "                results.append(result)\n",
        "                chunk_stats.append(chunk_stat)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  Error: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"모든 실험 완료!\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "elif not TESTSET_READY:\n",
        "    print(\"\\n Golden Testset을 먼저 작성해주세요!\")\n",
        "    print(f\"   파일 위치: {GOLDEN_TESTSET_PATH}\")\n",
        "else:\n",
        "    print(\"\\n 샘플 PDF 파일이 없습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9c6a426",
      "metadata": {},
      "source": [
        "## 7. 결과 분석 (Analysis)\n",
        "Chunk Size와 Table Strategy에 따른 Recall 변화를 시각화합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc739b18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 결과 분석\n",
        "df_results = pd.DataFrame(results)\n",
        "df_chunks = pd.DataFrame(chunk_stats)\n",
        "\n",
        "if not df_results.empty:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"[Result] 실험 결과 요약\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 1. Context Recall 비교 (핵심 메트릭)\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # 1-1. Recall by Config (Bar Chart)\n",
        "    pivot_recall = df_results.pivot(index=\"chunk_size\", columns=\"table_strategy\", values=\"context_recall\")\n",
        "    pivot_recall.plot(kind=\"bar\", ax=axes[0], title=\"Context Recall by Config\", colormap=\"viridis\")\n",
        "    axes[0].set_ylabel(\"Context Recall\")\n",
        "    axes[0].set_xlabel(\"Chunk Size\")\n",
        "    axes[0].legend(title=\"Table Strategy\")\n",
        "    axes[0].axhline(y=0.85, color='r', linestyle='--', label='Target (0.85)')\n",
        "    \n",
        "    # 1-2. Chunk Count by Config\n",
        "    pivot_chunks = df_results.pivot(index=\"chunk_size\", columns=\"table_strategy\", values=\"num_chunks\")\n",
        "    pivot_chunks.plot(kind=\"bar\", ax=axes[1], title=\"Number of Chunks\", colormap=\"plasma\")\n",
        "    axes[1].set_ylabel(\"Chunk Count\")\n",
        "    axes[1].set_xlabel(\"Chunk Size\")\n",
        "    \n",
        "    # 1-3. Latency by Config\n",
        "    pivot_latency = df_results.pivot(index=\"chunk_size\", columns=\"table_strategy\", values=\"latency_total\")\n",
        "    pivot_latency.plot(kind=\"bar\", ax=axes[2], title=\"Total Latency (s)\", colormap=\"coolwarm\")\n",
        "    axes[2].set_ylabel(\"Latency (seconds)\")\n",
        "    axes[2].set_xlabel(\"Chunk Size\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"../data/experiments/exp01_results.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # 2. 상세 결과 테이블\n",
        "    print(\"\\n[Details] 상세 결과:\")\n",
        "    print(df_results.to_string(index=False))\n",
        "    \n",
        "    # 3. 청크 통계\n",
        "    print(\"\\n[Stats] 청크 통계:\")\n",
        "    print(df_chunks[['config', 'num_chunks', 'avg_length', 'std_length']].to_string(index=False))\n",
        "    \n",
        "    # 4. 최적 설정 선택\n",
        "    best_idx = df_results['context_recall'].idxmax()\n",
        "    best_config = df_results.loc[best_idx]\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"[Best] 최적 설정 (Best Config)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"  Config: {best_config['config']}\")\n",
        "    print(f\"  Context Recall: {best_config['context_recall']:.4f}\")\n",
        "    print(f\"  Chunk Count: {best_config['num_chunks']}\")\n",
        "    print(f\"  Total Latency: {best_config['latency_total']:.2f}s\")\n",
        "    \n",
        "else:\n",
        "    print(\"[Warning] 결과가 없습니다. 실험을 먼저 실행하세요.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b910d8f0",
      "metadata": {},
      "source": [
        "## 8. 결론 및 선정 (Conclusion & Selection)\n",
        "\n",
        "### 8.1 실험 결과 분석\n",
        "- **최고 성능**: `chunk=500_table=layout` (Recall **0.7333**)\n",
        "- **Chunk Size**: 작은 청크(500)가 더 정밀한 검색 결과를 보여줌. (500 > 2000 > 1000)\n",
        "  - 500 Layout (0.73) vs 2000 Text (0.71)로 경합했으나, 500 Layout이 소폭 우세.\n",
        "- **Table Strategy**: `Layout` 전략이 500 청크에서 뚜렷한 이득을 줌 (+0.05 Recall vs Text).\n",
        "\n",
        "### 8.2 인사이트 (Findings)\n",
        "1. **Small & Structured**: RFP의 세부 요건을 찾을 때는 문맥을 너무 길게 잡는(2000) 것보다, **표 구조를 유지한 채 작게(500) 자르는 것**이 유리함.\n",
        "2. **Layout의 중요성**: 500 토큰 구간에서 Text(0.68) vs Layout(0.73) 차이는 큼. 표 태그/마크다운이 섞이더라도 구조적 의미가 보존될 때 LLM 판단력이 좋아짐.\n",
        "3. **Latency**: 청크 수가 많아(611개) Latency가 85초대로 다소 증가했으나, 정확도를 위해 감수할 만한 수준.\n",
        "\n",
        "### 8.3 최종 선정 (Final Decision)\n",
        "```yaml\n",
        "Selected Parameters:\n",
        "- Chunk Size: 500\n",
        "- Table Strategy: layout\n",
        "- 근거: Grid Search 최고 Recall (0.7333)\n",
        "```\n",
        "\n",
        "### 8.4 적용 계획\n",
        "- `configs/prod.yaml` 설정을 즉시 업데이트하여 운영 파이프라인에 반영.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6a37b40",
      "metadata": {},
      "source": [
        "## 9. 리포트 저장 (Save)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ee4860d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# 결과 저장\n",
        "if results:\n",
        "    report_path = \"../data/experiments/exp01_report.json\"\n",
        "    \n",
        "    # Best config 추출\n",
        "    best_idx = df_results['context_recall'].idxmax()\n",
        "    best = df_results.loc[best_idx].to_dict()\n",
        "    \n",
        "    report = {\n",
        "        \"meta\": {\n",
        "            \"experiment\": \"Exp-01 Chunking Optimization\",\n",
        "            \"version\": \"v2 (Human-labeled)\",\n",
        "            \"date\": datetime.now().isoformat(),\n",
        "            \"sample_file\": SAMPLE_FILE,\n",
        "            \"num_test_cases\": len(golden_df) if TESTSET_READY else 0,\n",
        "        },\n",
        "        \"best_config\": {\n",
        "            \"chunk_size\": int(best['chunk_size']),\n",
        "            \"table_strategy\": best['table_strategy'],\n",
        "            \"context_recall\": float(best['context_recall']),\n",
        "        },\n",
        "        \"results\": results,\n",
        "        \"chunk_stats\": chunk_stats,\n",
        "    }\n",
        "    \n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"[Saved] Report saved: {report_path}\")\n",
        "    \n",
        "    # CSV도 저장 (분석 편의)\n",
        "    df_results.to_csv(\"../data/experiments/exp01_results.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"[Saved] CSV saved: ../data/experiments/exp01_results.csv\")\n",
        "else:\n",
        "    print(\"[Warning] 저장할 결과가 없습니다.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}