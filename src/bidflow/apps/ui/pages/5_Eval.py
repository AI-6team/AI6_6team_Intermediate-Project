from bidflow.ingest.storage import DocumentStore
from bidflow.eval.ragas_runner import RagasRunner
from bidflow.apps.ui.auth import require_login
from langchain_core.documents import Document
import pandas as pd
import streamlit as st

st.set_page_config(page_title="Evaluation", page_icon="ğŸ“ˆ")

user_id = require_login()

st.title("System Evaluation (RAGAS)")

from bidflow.apps.ui.session import init_app_session
init_app_session(user_id=user_id)

if "current_doc_hash" not in st.session_state:
    st.warning("ë¨¼ì € 'Upload' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.")
    st.stop()

doc_hash = st.session_state["current_doc_hash"]
store = DocumentStore(user_id=user_id)
doc = store.load_document(doc_hash)

if not doc:
    st.error("ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

st.write(f"Target Document: **{doc.filename}**")

if not doc.chunks:
    st.error("ë¶„ì„ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.stop()

st.markdown("""
### RAGAS Metrics
- **Faithfulness**: LLMì˜ ë‹µë³€ì´ ì£¼ì–´ì§„ ë¬¸ë§¥(ê·¼ê±°)ì— ì¶©ì‹¤í•œì§€ (í™˜ê° ì—¬ë¶€)
- **Answer Relevancy**: ë‹µë³€ì´ ì§ˆë¬¸ì˜ ì˜ë„ì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€
""")

if st.button("Run Evaluation (ì•½ 1~2ë¶„ ì†Œìš”)"):
    with st.spinner("1. í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì¤‘... (Synthetic Generation)"):
        lc_docs = [
            Document(page_content=chunk.text, metadata=chunk.metadata)
            for chunk in doc.chunks
        ]

        runner = RagasRunner()
        testset = runner.generate_testset(lc_docs, test_size=3)
        st.success(f"í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì™„ë£Œ ({len(testset)} pairs)")

        display_cols = []
        for col in ["user_input", "question", "reference", "ground_truth"]:
            if col in testset.columns:
                display_cols.append(col)
        st.dataframe(testset[display_cols[:2]].head() if display_cols else testset.head())

    with st.spinner("2. í‰ê°€ ìˆ˜í–‰ ì¤‘... (Evaluating)"):
        results = runner.run_eval(testset)
        st.success("í‰ê°€ ì™„ë£Œ!")

        st.subheader("Evaluation Scores")

        metric_cols = []
        col_mapping = {}
        if "faithfulness" in results.columns:
            metric_cols.append("faithfulness")
            col_mapping["faithfulness"] = "Faithfulness"
        if "answer_relevancy" in results.columns:
            metric_cols.append("answer_relevancy")
            col_mapping["answer_relevancy"] = "Answer Relevancy"
        if "response_relevancy" in results.columns:
            metric_cols.append("response_relevancy")
            col_mapping["response_relevancy"] = "Response Relevancy"

        if metric_cols:
            avg_scores = results[metric_cols].mean()

            cols = st.columns(len(metric_cols))
            for i, col_name in enumerate(metric_cols):
                score = avg_scores[col_name]
                display_name = col_mapping.get(col_name, col_name)
                cols[i].metric(display_name, f"{score:.2f}" if pd.notna(score) else "N/A")

            st.bar_chart(avg_scores)
        else:
            st.warning("í‰ê°€ ë©”íŠ¸ë¦­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        st.subheader("Detailed Results")
        st.dataframe(results)
