import concurrent.futures
import hashlib
import os
import re
from typing import Optional
from bidflow.domain.models import RFPDocument
from bidflow.parsing.pdf_parser import PDFParser
from bidflow.parsing.hwp_parser import HWPParser
from bidflow.parsing.docx_parser import DOCXParser
from bidflow.parsing.hwpx_parser import HWPXParser
from bidflow.ingest.storage import DocumentStore, VectorStoreManager, StorageRegistry
from bidflow.security.rails.input_rail import InputRail, SecurityException
from bidflow.security.pii_filter import PIIFilter

class RFPLoader:
    """
    íŒŒì¼ ì—…ë¡œë“œ -> íŒŒì‹± -> ì €ì¥ -> ë²¡í„°DB ì ì¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” íŒŒì‚¬ë“œ
    """
    def __init__(self, user_id: str = "global"):
        from bidflow.core.config import get_config

        self.config = get_config("dev") # Default to dev for MVP
        self.user_id = user_id or "global"
        self.default_tenant_id = self.user_id if self.user_id != "global" else "default"
        self.default_group_id = "general"

        registry = StorageRegistry(self.config)
        self.doc_store = DocumentStore(user_id=self.user_id, registry=registry)
        self.vec_manager = VectorStoreManager(user_id=self.user_id, registry=registry)
        self.pdf_parser = PDFParser()
        self.hwp_parser = HWPParser()
        self.docx_parser = DOCXParser()
        self.hwpx_parser = HWPXParser()
        self.input_rail = InputRail()
        self.pii_filter = PIIFilter()

    def process_file(
        self,
        file_obj,
        filename: str,
        chunk_size: int = None,
        chunk_overlap: int = None,
        table_strategy: str = None,
        tenant_id: Optional[str] = None,
        user_id: Optional[str] = None,
        group_id: Optional[str] = None,
        max_file_size_mb: int = 50,
        parsing_timeout: int = 300,
        single_document_policy: bool = True,
    ) -> str:
        """
        Streamlit UploadedFile ê°ì²´ë¥¼ ë°›ì•„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        ë°˜í™˜ê°’: ìƒì„±ëœ doc_hash
        """
        effective_tenant_id = tenant_id or self.default_tenant_id
        effective_user_id = user_id or (self.user_id if self.user_id != "global" else "system")
        effective_group_id = group_id or self.default_group_id

        # [Policy] ë‹¨ì¼ ë¬¸ì„œ ëª¨ë“œ: ìƒˆ ì—…ë¡œë“œ ì „ ê¸°ì¡´ í…Œë„ŒíŠ¸ ë°ì´í„° ì œê±°
        if single_document_policy:
            self.purge_tenant(effective_tenant_id)

        filename = os.path.basename(filename)
        ext = os.path.splitext(filename)[1].lower()

        # [Security] ì‹¤í–‰ íŒŒì¼/ìŠ¤í¬ë¦½íŠ¸ ì°¨ë‹¨
        blocked_exts = {".exe", ".sh", ".bat", ".cmd", ".js", ".vbs", ".py", ".php", ".jsp", ".asp", ".dll", ".bin"}
        if ext in blocked_exts:
            raise ValueError(f"ë³´ì•ˆ ì •ì±… ìœ„ë°˜: ì‹¤í–‰ íŒŒì¼ ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸({ext})ëŠ” ì—…ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        supported_exts = {".pdf", ".hwp", ".docx", ".hwpx"}
        if ext not in supported_exts:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}. (ì§€ì› í˜•ì‹: {', '.join(sorted(supported_exts))})")

        # Config Defaults
        chunk_size = chunk_size or self.config.parsing.chunk_size or 500
        chunk_overlap = chunk_overlap or self.config.parsing.chunk_overlap or 50
        table_strategy = table_strategy or self.config.parsing.table_strategy or "text"

        # [Security] íŒŒì¼ í¬ê¸° ì œí•œ
        limit_bytes = max_file_size_mb * 1024 * 1024
        if hasattr(file_obj, "size") and file_obj.size > limit_bytes:
            raise ValueError(f"File size ({file_obj.size} bytes) exceeds limit of {max_file_size_mb} MB")

        # 1. íŒŒì¼ ì„ì‹œ ì €ì¥ & í•´ì‹œ ê³„ì‚°
        temp_dir = os.path.join("data", effective_tenant_id, "temp")
        os.makedirs(temp_dir, exist_ok=True)
        temp_path = os.path.join(temp_dir, filename)

        content = file_obj.read()
        if len(content) > limit_bytes:
            raise ValueError(f"File size ({len(content)} bytes) exceeds limit of {max_file_size_mb} MB")

        # [Security] Magic Number Check (í™•ì¥ì ìœ„ë³€ì¡° íƒì§€)
        magic_signatures = {
            ".pdf": [b"%PDF"],
            ".hwp": [b"\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1"], # OLE Compound File
            ".docx": [b"\x50\x4B\x03\x04"], # ZIP Header
            ".hwpx": [b"\x50\x4B\x03\x04"], # ZIP Header
        }
        if ext in magic_signatures:
            is_valid_signature = any(content.startswith(sig) for sig in magic_signatures[ext])
            if not is_valid_signature:
                raise ValueError(f"ë³´ì•ˆ ê²½ê³ : íŒŒì¼ ë‚´ìš©ì´ í™•ì¥ì({ext})ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (Magic Number Mismatch)")

        doc_hash = hashlib.md5(content).hexdigest()

        with open(temp_path, "wb") as f:
            f.write(content)

        # 2. íŒŒì‹± (í™•ì¥ì ë¶„ê¸° + íƒ€ì„ì•„ì›ƒ)
        def _execute_parser():
            if ext == ".pdf":
                parsed_chunks = self.pdf_parser.parse(
                    temp_path,
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    table_strategy=table_strategy,
                )
                parsed_tables = self.pdf_parser.extract_tables(temp_path)
                return parsed_chunks, parsed_tables
            if ext == ".hwp":
                print("ğŸ”’ Performing Deep Scan on HWP structure...")
                if self.hwp_parser.deep_scan(temp_path, self.input_rail.patterns):
                    raise SecurityException("Banned pattern detected in HWP hidden stream (Deep Scan)")
                return self.hwp_parser.parse(temp_path), []
            if ext == ".docx":
                parsed_chunks = self.docx_parser.parse(temp_path, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
                parsed_tables = self.docx_parser.extract_tables(temp_path)
                return parsed_chunks, parsed_tables
            if ext == ".hwpx":
                parsed_chunks = self.hwpx_parser.parse(temp_path, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
                parsed_tables = self.hwpx_parser.extract_tables(temp_path)
                return parsed_chunks, parsed_tables
            return [], []

        try:
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(_execute_parser)
                chunks, tables = future.result(timeout=parsing_timeout)
        except concurrent.futures.TimeoutError as exc:
            raise TimeoutError(
                f"ë³´ì•ˆ ê²½ê³ : íŒŒì¼ íŒŒì‹± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤ ({parsing_timeout}ì´ˆ). ë³µì¡í•˜ê±°ë‚˜ ì•…ì˜ì ì¸ íŒŒì¼ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ) from exc

        # [Security] í…ìŠ¤íŠ¸ ì •ì œ + PII ë§ˆìŠ¤í‚¹ + ìœ íš¨ ê¸¸ì´ í•„í„°
        valid_chunks = []
        for chunk in chunks:
            cleaned_text = self._sanitize_text(chunk.text)
            masked_text = self._mask_pii(cleaned_text)
            if cleaned_text != masked_text:
                print(f"ğŸ”’ [PII Masking] Detected & Masked personal info in Chunk {chunk.chunk_id} (Page {chunk.page_no})")
            chunk.text = masked_text
            if len(chunk.text.strip()) >= 10:
                valid_chunks.append(chunk)
        chunks = valid_chunks

        if not chunks:
            raise ValueError(f"ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨: ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (íŒŒì¼ëª…: {filename})")

        # [Security] ì¸ì ì…˜ ì „ì²´ ìŠ¤ìº” (Ingest ë‹¨ê³„ ë°©ì–´)
        print(f"ğŸ”’ Scanning {len(chunks)} chunks for Prompt Injection...")
        for i, chunk in enumerate(chunks):
            # Debug: ìŠ¤ìº” ì¤‘ì¸ í…ìŠ¤íŠ¸ í™•ì¸
            print(f"[Chunk {i}] Preview: {chunk.text[:50]}...")
            self.input_rail.check(chunk.text)

        # 3. RFPDocument ìƒì„±
        doc = RFPDocument(
            id=doc_hash,
            filename=filename,
            file_path=temp_path,
            doc_hash=doc_hash,
            chunks=chunks,
            tables=tables,
            status="READY"
        )

        # 4. ì €ì¥ (JSON + Chroma)
        self.doc_store.save_document(doc, tenant_id=effective_tenant_id)
        self.vec_manager.ingest_document(
            doc,
            tenant_id=effective_tenant_id,
            user_id=effective_user_id,
            group_id=effective_group_id,
        )

        print(f"Processed {filename} -> {doc_hash} (Chunks: {len(chunks)})")
        return doc_hash

    def purge_tenant(self, tenant_id: str):
        """
        íŠ¹ì • í…Œë„ŒíŠ¸ì˜ ëª¨ë“  ë°ì´í„°(íŒŒì¼ + ë²¡í„°)ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
        """
        print(f"Purging data for tenant: {tenant_id}...")
        self.doc_store.purge_tenant_data(tenant_id)
        self.vec_manager.delete_tenant_data(tenant_id)
        print(f"Purge complete for tenant: {tenant_id}")

    def _sanitize_text(self, text: str) -> str:
        """
        ë°”ì´ë„ˆë¦¬/ê¹¨ì§„ ë¬¸ì ë…¸ì´ì¦ˆë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
        í—ˆìš© ë²”ìœ„: í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µí†µ íŠ¹ìˆ˜ë¬¸ì, CJK ê¸°í˜¸ ì¼ë¶€
        """
        text = "".join(ch for ch in text if ch == "\n" or ch == "\t" or ch >= " ")
        pattern = (
            r"[^\u0009\u000A\u0020-\u007E\uAC00-\uD7A3\u00A0-\u00FF"
            r"\u1100-\u11FF\u3130-\u318F\u3000-\u303F\u2000-\u20CF"
            r"\u2100-\u21FF\u2500-\u257F\uFF00-\uFFEF]+"
        )
        return re.sub(pattern, "", text)

    def _mask_pii(self, text: str) -> str:
        """
        ì¤‘ì•™í™”ëœ PIIFilterë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ë‚´ PIIë¥¼ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤.
        """
        return self.pii_filter.sanitize(text)
