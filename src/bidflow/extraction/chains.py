from typing import Dict, Any, List, Optional
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from langfuse import observe
import os

from bidflow.domain.models import Evidence, ExtractionSlot


def _get_llm_model() -> str:
    """dev.yamlì˜ model.llm ê°’ì„ ì½ì–´ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        from bidflow.core.config import get_config
        cfg = get_config("dev")
        return (cfg.model.llm if cfg.model and cfg.model.llm else None) or "gpt-5-mini"
    except Exception:
        return "gpt-5-mini"


class ExtractionChain:
    """ê¸°ë³¸ ì¶”ì¶œ ì²´ì¸ í´ëž˜ìŠ¤"""

    def __init__(self, model_name: str = None):
        if model_name is None:
            model_name = _get_llm_model()
        # streaming=TrueëŠ” PydanticOutputParserì™€ í˜¸í™˜ë˜ì§€ ì•Šì•„ ë¹„í™œì„±í™”
        self.llm = ChatOpenAI(model=model_name, temperature=0, streaming=False)

    def _load_prompt(self, prompt_path: str) -> str:
        base_dir = os.path.dirname(__file__)
        full_path = os.path.join(base_dir, prompt_path)
        with open(full_path, "r", encoding="utf-8") as f:
            return f.read()


# --- G1: ê¸°ë³¸ ì •ë³´ ---

class G1Result(BaseModel):
    project_name: ExtractionSlot
    issuer: ExtractionSlot
    period: ExtractionSlot
    budget: ExtractionSlot


class G1Chain(ExtractionChain):
    @observe(name="G1_Basic_Info")
    def run(self, context_text: str) -> G1Result:
        prompt_text = self._load_prompt("prompts/g1_basic.md")
        parser = PydanticOutputParser(pydantic_object=G1Result)

        prompt = ChatPromptTemplate.from_messages([
            ("system", prompt_text),
            ("user", "Context: {context}\n\n{format_instructions}")
        ])

        chain = prompt | self.llm | parser
        try:
            return chain.invoke({
                "context": context_text,
                "format_instructions": parser.get_format_instructions()
            })
        except Exception as e:
            print(f"âŒ Extraction Failed (G1): {e}")
            raw_chain = prompt | self.llm
            raw_res = raw_chain.invoke({
                "context": context_text,
                "format_instructions": parser.get_format_instructions()
            })
            print(f"ðŸ” Raw LLM Output: {raw_res.content}")
            raise RuntimeError(f"G1 ì¶”ì¶œ ì‹¤íŒ¨: {e}") from e


# --- G2: ì¼ì • ---

class G2Result(BaseModel):
    submission_deadline: ExtractionSlot
    briefing_date: ExtractionSlot
    qna_period: ExtractionSlot


class G2Chain(ExtractionChain):
    @observe(name="G2_Schedule")
    def run(self, context_text: str, project_name: str, period: str) -> G2Result:
        prompt_text = self._load_prompt("prompts/g2_schedule.md")
        parser = PydanticOutputParser(pydantic_object=G2Result)

        prompt = ChatPromptTemplate.from_messages([
            ("system", prompt_text),
            ("user", "Context: {context}\n\n{format_instructions}")
        ])

        chain = prompt | self.llm | parser
        return chain.invoke({
            "context": context_text,
            "project_name": project_name,
            "period": period,
            "format_instructions": parser.get_format_instructions()
        })


# --- G3: ìžê²© ìš”ê±´ ---

class G3Result(BaseModel):
    required_licenses: ExtractionSlot
    region_restriction: ExtractionSlot
    financial_credit: ExtractionSlot
    restrictions: ExtractionSlot


class G3Chain(ExtractionChain):
    @observe(name="G3_Qualification")
    def run(self, context_text: str, project_name: str, issuer: str) -> G3Result:
        prompt_text = self._load_prompt("prompts/g3_qual.md")
        parser = PydanticOutputParser(pydantic_object=G3Result)

        prompt = ChatPromptTemplate.from_messages([
            ("system", prompt_text),
            ("user", "Context: {context}\n\n{format_instructions}")
        ])

        chain = prompt | self.llm | parser
        return chain.invoke({
            "context": context_text,
            "project_name": project_name,
            "issuer": issuer,
            "format_instructions": parser.get_format_instructions()
        })


# --- G4: ë°°ì í‘œ ---

class ScoredItem(BaseModel):
    category: str
    item: str
    score: float


class G4Result(BaseModel):
    items: List[ScoredItem]


class G4Chain(ExtractionChain):
    @observe(name="G4_Score")
    def run(self, context_text: str) -> G4Result:
        prompt_text = self._load_prompt("prompts/g4_score.md")
        parser = PydanticOutputParser(pydantic_object=G4Result)

        prompt = ChatPromptTemplate.from_messages([
            ("system", prompt_text),
            ("user", "Context: {context}\n\n{format_instructions}")
        ])

        chain = prompt | self.llm | parser
        return chain.invoke({
            "context": context_text,
            "format_instructions": parser.get_format_instructions()
        })
